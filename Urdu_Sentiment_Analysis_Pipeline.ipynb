{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMaZyzQOPZXs",
        "outputId": "95059f66-7435-4287-bd1b-1d55afab522a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffurdu_text', 'is_sarcastic', '', '', '', '', '', '']\n",
            "['ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†Ø§ÛÛŒÛ’ ğŸ˜ğŸ˜ğŸ˜ğŸ¤£', '1', '', '', '', '', '', '']\n",
            "['Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚºğŸ˜‚ğŸ˜‚', '1', '', '', '', '', '', '']\n",
            "['Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´Ù† Ú©ÛŒ Ú©Ø±Ø¯Ø§Ø± Ú©Ø´ÛŒ Ø§ÙˆØ±Ø§Ø³ Ù¾Ø±Ø¨Ú¾ÙˆÙ†Ú©Ù†Ø§ÛÛ’Ø¢Ù¾ Ø®ÙˆØ´Ø§Ù…Ø¯Ú¯Ø±ÛŒ ÙˆÚ†Ø§Ù¾Ù„ÙˆØ³ÛŒ Ø³Û’Ø§ÙˆØ±Ú©ØªÙ†ÛŒ Ø¯ÙˆÙ„Øª Ú©Ù…Ø§Ù†Ø§Ú†Ø§ÛØªÛ’ÛÛŒÚº Ù…ÙˆÙ¹Ø±Ø³Ø§Ø¦ÛŒÚ©Ù„ Ø³Û’Ù¾ÛŒØ¬Ø§Ø±Ùˆ Ù¾Ø±Ø§ÚˆÙˆ ØªÚ© Ú©Û’Ø³ÙØ±Ù…ÛŒÚº Ø¶Ù…ÛŒØ±Ú©ÛŒ Ù„Ø§Ø´ Ø³Û’Ø§Ù¹Ú¾ØªÛŒ Ø¨Ø¯Ø¨ÙˆØ¢Ù¾ Ú©ÛŒ Ù†Ø§Ú© Ø¨Ù†Ø¯ Ù†ÛÛŒÚº Ú©Ø±ØªÛŒ ÛÛ’ ğŸ™Ù†ÙˆÙ¹ Ø¢Ù¾ Ø³Ø¨ Ø³Û’Ø§Ù„ØªØ¬Ø§Ú¯Ø²Ø§Ø±Ø´ ÛÛ’ÛÙ…ÛŒÚº Ø¨Ú¾ÛŒ ÙØ§Ù„ÙˆÚ©Ø±ÛŒÚº Ø´Ú©Ø±ÛŒÛ', '0', '', '', '', '', '', '']\n",
            "['Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜', '0', '', '', '', '', '', '']\n",
            "[\" `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±ğŸ˜\", '1', '', '', '', '', '', '']\n",
            "[' Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº ğŸ’”ğŸ”¥', '1', '', '', '', '', '', '']\n",
            "['Ø§Ù†Ø³Ø§Úº Ú©Ùˆ ØªÚ¾Ú©Ø§ Ø¯ÛŒØªØ§ ÛÛ’ Ø³ÙˆÚ†ÙˆÚº Ú©Ø§ Ø³ÙØ± Ø¨Ú¾ÛŒ ... ğŸğŸ¥€', '0', '', '', '', '', '', '']\n",
            "['Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ØµØ§Ø­Ø¨ ÙˆÛŒÙ„ÚˆÙ†ğŸ‘ğŸ˜Š', '0', '', '', '', '', '', '']\n",
            "['ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ ÛÛ’ Ø§Ø³ Ø¢Ø±Û’ Ù„Ú¯Ø§ ÛÙˆÛŒØ§ ÛÛ’ğŸ˜‚ğŸ˜‚ ØªØ³ÛŒ ØªÛ’ Ù¾Ú©Û’ Ù†Ø¬ÙˆÙ…ÛŒ ÛÙˆ Ø§Ø³ÛŒ Ù…Ù†Ù†Ø¯Û’ ÛØ§Úº', '1', '', '', '', '', '', '']\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "file_path = 'urdu_sarcastic_dataset.csv'  # Update this with the actual path to your CSV file\n",
        "\n",
        "# Open the CSV file in read mode\n",
        "with open(file_path, 'r') as csv_file:\n",
        "    # Create a CSV reader object to read the file\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "\n",
        "    # Loop through the first 10 rows and print each row\n",
        "    for i, row in enumerate(csv_reader):\n",
        "        # If the index reaches 10, exit the loop\n",
        "        if i >= 10:\n",
        "            break\n",
        "        # Print the current row\n",
        "        print(row)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a list of Urdu stopwords (can be extended based on the dataset)\n",
        "urdu_stopwords = [\n",
        "    \"Ø§ÙˆØ±\", \"ÛŒÛ\", \"Ú©Û\", \"ÛÛ’\", \"Ù…ÛŒÚº\", \"Ú©ÛŒ\", \"Ú©Ø§\", \"Ú©Û’\", \"Ú©Ùˆ\", \"Ù†Û’\", \"ÛÙˆÚº\", \"ÛÛŒÚº\",\n",
        "    \"ØªÚ¾Ø§\", \"ØªÚ¾ÛŒ\", \"ØªÚ¾Û’\", \"ÛÙˆ\", \"ÛÙˆØ§\", \"ÛÙˆØ¦\", \"ÛÙˆØ¦ÛŒ\", \"Ù¾Ø±\", \"Ø³Û’\", \"Ú©Û’\", \"ÛÙ…\", \"ØªÙ…\"\n",
        "]\n",
        "# Example words that may carry sentiment but are commonly considered stopwords\n",
        "sentiment_stopwords = [\"Ù†ÛÛŒÚº\", \"Ø¨Ø±Ø§\", \"Ø§Ú†Ú¾Ø§\", \"ØºÙ„Ø·\"]\n",
        "def remove_stopwords(text, stopwords, sentiment_words=None):\n",
        "    words = text.split()  # Split text into individual words\n",
        "    filtered_words = [\n",
        "        word for word in words\n",
        "        if word not in stopwords or (sentiment_words and word in sentiment_words)\n",
        "    ]  # Remove stopwords but keep sentiment words\n",
        "    return \" \".join(filtered_words)  # Join words back into a string\n",
        "def process_file(file_path, stopwords, sentiment_words=None, encoding='utf-8'):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding=encoding) as csv_file:\n",
        "            csv_reader = csv.reader(csv_file)\n",
        "            # Process a limited number of rows, adjust as necessary\n",
        "            for i, row in enumerate(csv_reader):\n",
        "                if i >= 20:  # Adjust this limit based on dataset size\n",
        "                    break\n",
        "                if row:  # Ensure the row is not empty\n",
        "                    text = row[0]  # Assuming text is in the first column\n",
        "                    processed_text = remove_stopwords(text, stopwords, sentiment_words)\n",
        "\n",
        "                    # Display original and processed text\n",
        "                    print(f\"Original: {text}\")\n",
        "                    print(f\"Processed: {processed_text}\")\n",
        "                    print(\"---\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File {file_path} not found.\")\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"Error: Encoding issue with {file_path}. Try adjusting the encoding.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "# Main function to run the file processing\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to the CSV file\n",
        "    file_path = 'urdu_sarcastic_dataset.csv'\n",
        "    # Start processing the file\n",
        "    process_file(file_path, urdu_stopwords, sentiment_words=sentiment_stopwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SebhPQ66VicA",
        "outputId": "564224f5-882a-4c86-e9c6-0729646bc017"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: ï»¿urdu_text\n",
            "Processed: ï»¿urdu_text\n",
            "---\n",
            "Original: ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†Ø§ÛÛŒÛ’ ğŸ˜ğŸ˜ğŸ˜ğŸ¤£\n",
            "Processed: ğŸ¤£ğŸ˜‚ğŸ˜‚ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†Ø§ÛÛŒÛ’ ğŸ˜ğŸ˜ğŸ˜ğŸ¤£\n",
            "---\n",
            "Original: Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚºğŸ˜‚ğŸ˜‚\n",
            "Processed: Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚºğŸ˜‚ğŸ˜‚\n",
            "---\n",
            "Original: Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´Ù† Ú©ÛŒ Ú©Ø±Ø¯Ø§Ø± Ú©Ø´ÛŒ Ø§ÙˆØ±Ø§Ø³ Ù¾Ø±Ø¨Ú¾ÙˆÙ†Ú©Ù†Ø§ÛÛ’Ø¢Ù¾ Ø®ÙˆØ´Ø§Ù…Ø¯Ú¯Ø±ÛŒ ÙˆÚ†Ø§Ù¾Ù„ÙˆØ³ÛŒ Ø³Û’Ø§ÙˆØ±Ú©ØªÙ†ÛŒ Ø¯ÙˆÙ„Øª Ú©Ù…Ø§Ù†Ø§Ú†Ø§ÛØªÛ’ÛÛŒÚº Ù…ÙˆÙ¹Ø±Ø³Ø§Ø¦ÛŒÚ©Ù„ Ø³Û’Ù¾ÛŒØ¬Ø§Ø±Ùˆ Ù¾Ø±Ø§ÚˆÙˆ ØªÚ© Ú©Û’Ø³ÙØ±Ù…ÛŒÚº Ø¶Ù…ÛŒØ±Ú©ÛŒ Ù„Ø§Ø´ Ø³Û’Ø§Ù¹Ú¾ØªÛŒ Ø¨Ø¯Ø¨ÙˆØ¢Ù¾ Ú©ÛŒ Ù†Ø§Ú© Ø¨Ù†Ø¯ Ù†ÛÛŒÚº Ú©Ø±ØªÛŒ ÛÛ’ ğŸ™Ù†ÙˆÙ¹ Ø¢Ù¾ Ø³Ø¨ Ø³Û’Ø§Ù„ØªØ¬Ø§Ú¯Ø²Ø§Ø±Ø´ ÛÛ’ÛÙ…ÛŒÚº Ø¨Ú¾ÛŒ ÙØ§Ù„ÙˆÚ©Ø±ÛŒÚº Ø´Ú©Ø±ÛŒÛ\n",
            "Processed: Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´Ù† Ú©Ø±Ø¯Ø§Ø± Ú©Ø´ÛŒ Ø§ÙˆØ±Ø§Ø³ Ù¾Ø±Ø¨Ú¾ÙˆÙ†Ú©Ù†Ø§ÛÛ’Ø¢Ù¾ Ø®ÙˆØ´Ø§Ù…Ø¯Ú¯Ø±ÛŒ ÙˆÚ†Ø§Ù¾Ù„ÙˆØ³ÛŒ Ø³Û’Ø§ÙˆØ±Ú©ØªÙ†ÛŒ Ø¯ÙˆÙ„Øª Ú©Ù…Ø§Ù†Ø§Ú†Ø§ÛØªÛ’ÛÛŒÚº Ù…ÙˆÙ¹Ø±Ø³Ø§Ø¦ÛŒÚ©Ù„ Ø³Û’Ù¾ÛŒØ¬Ø§Ø±Ùˆ Ù¾Ø±Ø§ÚˆÙˆ ØªÚ© Ú©Û’Ø³ÙØ±Ù…ÛŒÚº Ø¶Ù…ÛŒØ±Ú©ÛŒ Ù„Ø§Ø´ Ø³Û’Ø§Ù¹Ú¾ØªÛŒ Ø¨Ø¯Ø¨ÙˆØ¢Ù¾ Ù†Ø§Ú© Ø¨Ù†Ø¯ Ù†ÛÛŒÚº Ú©Ø±ØªÛŒ ğŸ™Ù†ÙˆÙ¹ Ø¢Ù¾ Ø³Ø¨ Ø³Û’Ø§Ù„ØªØ¬Ø§Ú¯Ø²Ø§Ø±Ø´ ÛÛ’ÛÙ…ÛŒÚº Ø¨Ú¾ÛŒ ÙØ§Ù„ÙˆÚ©Ø±ÛŒÚº Ø´Ú©Ø±ÛŒÛ\n",
            "---\n",
            "Original: Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜\n",
            "Processed: Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜\n",
            "---\n",
            "Original:  `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±ğŸ˜\n",
            "Processed: `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±ğŸ˜\n",
            "---\n",
            "Original:  Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº ğŸ’”ğŸ”¥\n",
            "Processed: Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ğŸ’”ğŸ”¥\n",
            "---\n",
            "Original: Ø§Ù†Ø³Ø§Úº Ú©Ùˆ ØªÚ¾Ú©Ø§ Ø¯ÛŒØªØ§ ÛÛ’ Ø³ÙˆÚ†ÙˆÚº Ú©Ø§ Ø³ÙØ± Ø¨Ú¾ÛŒ ... ğŸğŸ¥€\n",
            "Processed: Ø§Ù†Ø³Ø§Úº ØªÚ¾Ú©Ø§ Ø¯ÛŒØªØ§ Ø³ÙˆÚ†ÙˆÚº Ø³ÙØ± Ø¨Ú¾ÛŒ ... ğŸğŸ¥€\n",
            "---\n",
            "Original: Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ØµØ§Ø­Ø¨ ÙˆÛŒÙ„ÚˆÙ†ğŸ‘ğŸ˜Š\n",
            "Processed: Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ØµØ§Ø­Ø¨ ÙˆÛŒÙ„ÚˆÙ†ğŸ‘ğŸ˜Š\n",
            "---\n",
            "Original: ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ ÛÛ’ Ø§Ø³ Ø¢Ø±Û’ Ù„Ú¯Ø§ ÛÙˆÛŒØ§ ÛÛ’ğŸ˜‚ğŸ˜‚ ØªØ³ÛŒ ØªÛ’ Ù¾Ú©Û’ Ù†Ø¬ÙˆÙ…ÛŒ ÛÙˆ Ø§Ø³ÛŒ Ù…Ù†Ù†Ø¯Û’ ÛØ§Úº\n",
            "Processed: ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ Ø§Ø³ Ø¢Ø±Û’ Ù„Ú¯Ø§ ÛÙˆÛŒØ§ ÛÛ’ğŸ˜‚ğŸ˜‚ ØªØ³ÛŒ ØªÛ’ Ù¾Ú©Û’ Ù†Ø¬ÙˆÙ…ÛŒ Ø§Ø³ÛŒ Ù…Ù†Ù†Ø¯Û’ ÛØ§Úº\n",
            "---\n",
            "Original: ÛŒÛ Ø³Ù…Ø¬Ú¾ØªÛ’ ÛÛŒÚº Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’ ğŸ˜‚ğŸ˜‚ğŸ˜‚\n",
            "Processed: Ø³Ù…Ø¬Ú¾ØªÛ’ Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’ ğŸ˜‚ğŸ˜‚ğŸ˜‚\n",
            "---\n",
            "Original: ØªØ³ÛŒ Ù„Ú‘Ø§ÚºÙ”ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§ÚˆÛŒ Ú©ÛŒ ğŸ˜‚ğŸ˜‚ğŸ˜‚\n",
            "Processed: ØªØ³ÛŒ Ù„Ú‘Ø§ÚºÙ”ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§ÚˆÛŒ ğŸ˜‚ğŸ˜‚ğŸ˜‚\n",
            "---\n",
            "Original: Ù¾Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’..ØŸØŸğŸ˜†ğŸ™„\n",
            "Processed: Ù¾Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’..ØŸØŸğŸ˜†ğŸ™„\n",
            "---\n",
            "Original: Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ ÛÛ’ Ø§Ù„Ùˆ Ø¯ÙˆØ³Ùˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ 80Ø±ÙˆÙ¾Û’ Ú¯Ø² ğŸ˜­ğŸ˜­ğŸ˜­ ISPR\n",
            "Processed: Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ Ø§Ù„Ùˆ Ø¯ÙˆØ³Ùˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ 80Ø±ÙˆÙ¾Û’ Ú¯Ø² ğŸ˜­ğŸ˜­ğŸ˜­ ISPR\n",
            "---\n",
            "Original: ğŸ˜Ø¹Ø´Ù‚ Ø¬Ø¨ ØªÙ… Ú©Ùˆ Ø±Ø§Ø³ Ø¢Û“ Ú¯Ø§ ğŸ’”Ø²Ø®Ù… Ú©Ú¾Ø§Ù¶ Ú¯Û’ ğŸ˜ŠÙ…ÙØ³Ú©Ø±Ø§Ù¶ Ú¯Û’\n",
            "Processed: ğŸ˜Ø¹Ø´Ù‚ Ø¬Ø¨ Ø±Ø§Ø³ Ø¢Û“ Ú¯Ø§ ğŸ’”Ø²Ø®Ù… Ú©Ú¾Ø§Ù¶ Ú¯Û’ ğŸ˜ŠÙ…ÙØ³Ú©Ø±Ø§Ù¶ Ú¯Û’\n",
            "---\n",
            "Original: Ú†ÙˆÙ†Ø§ Ø§ÛŒØ³Ø§ ÛÛŒ ÛÙˆØªØ§ ğŸ˜‚\n",
            "Processed: Ú†ÙˆÙ†Ø§ Ø§ÛŒØ³Ø§ ÛÛŒ ÛÙˆØªØ§ ğŸ˜‚\n",
            "---\n",
            "Original: Ø®Ø§ØªÙ…_Ø§Ù„Ù†Ø¨ÛŒÛŒÙ†_Ù…Ø­Ù…Ø¯ï·º Surat 73 Ø³ÙˆØ±Ø© Ø§Ù„Ù…Ø²Ù…Ù„ Ayt 20 Ø­ØµÛ5 ØªØ±Ø¬Ù…Û Ø§ÙˆØ± Ú©Ú†Ú¾ Ø§Ù„Ù„Û Ú©ÛŒ Ø±Ø§Û Ù…ÛŒÚº Ù„Ú‘ØªÛ’ ÛÙˆÚº Ú¯Û’ ØªÙˆ Ø¬ØªÙ†Ø§ Ù‚Ø±Ø¢Ù† Ù…ÛŒØ³Ø± ÛÙˆ Ù¾Ú‘Ú¾ÙˆØ§ÙˆØ±Ù†Ù…Ø§Ø² Ù‚Ø§Ø¦Ù… Ø±Ú©Ú¾ÙˆØ§ÙˆØ±Ø²Ú©ÙˆÙ°Ø© Ø¯ÙˆØ§ÙˆØ± Ø§Ù„Ù„Û Ú©Ùˆ Ø§Ú†Ú¾Ø§ Ù‚Ø±Ø¶ Ø¯Ùˆ ğŸŒ¹ğŸŒ¹ Ø¯Ø±ÙˆØ¯_ÙˆÙ‚Ø±Ø¢Ù† ØµÙÙ„ÙÙ‘ÛŒ Ø§Ù„Ù„ÛÙ Ø¹ÙÙ„Ù°ÛŒ Ø­ÙØ¨ÙÛŒÙ’Ø¨ÙÛÙ– Ø³ÛŒÙÙ‘Ø¯ÙÙ†ÙØ§ Ù…ÙØ­ÙÙ…ÙÙ‘Ø¯Ù ÙˆÙÙ‘Ø¢Ù„ÙÛÙ– ÙˆÙØµÙØ­Ù’Ø¨ÙÛÙ– ÙˆÙØ³ÙÙ„ÙÙ‘Ù…Ù’\n",
            "Processed: Ø®Ø§ØªÙ…_Ø§Ù„Ù†Ø¨ÛŒÛŒÙ†_Ù…Ø­Ù…Ø¯ï·º Surat 73 Ø³ÙˆØ±Ø© Ø§Ù„Ù…Ø²Ù…Ù„ Ayt 20 Ø­ØµÛ5 ØªØ±Ø¬Ù…Û Ú©Ú†Ú¾ Ø§Ù„Ù„Û Ø±Ø§Û Ù„Ú‘ØªÛ’ Ú¯Û’ ØªÙˆ Ø¬ØªÙ†Ø§ Ù‚Ø±Ø¢Ù† Ù…ÛŒØ³Ø± Ù¾Ú‘Ú¾ÙˆØ§ÙˆØ±Ù†Ù…Ø§Ø² Ù‚Ø§Ø¦Ù… Ø±Ú©Ú¾ÙˆØ§ÙˆØ±Ø²Ú©ÙˆÙ°Ø© Ø¯ÙˆØ§ÙˆØ± Ø§Ù„Ù„Û Ø§Ú†Ú¾Ø§ Ù‚Ø±Ø¶ Ø¯Ùˆ ğŸŒ¹ğŸŒ¹ Ø¯Ø±ÙˆØ¯_ÙˆÙ‚Ø±Ø¢Ù† ØµÙÙ„ÙÙ‘ÛŒ Ø§Ù„Ù„ÛÙ Ø¹ÙÙ„Ù°ÛŒ Ø­ÙØ¨ÙÛŒÙ’Ø¨ÙÛÙ– Ø³ÛŒÙÙ‘Ø¯ÙÙ†ÙØ§ Ù…ÙØ­ÙÙ…ÙÙ‘Ø¯Ù ÙˆÙÙ‘Ø¢Ù„ÙÛÙ– ÙˆÙØµÙØ­Ù’Ø¨ÙÛÙ– ÙˆÙØ³ÙÙ„ÙÙ‘Ù…Ù’\n",
            "---\n",
            "Original: Ø§Ø¨ Ø¨Ø³ Ø¨Ú¾ÛŒ Ú©Ø±Ùˆ Ø¨ÛŒÚ†Ø§Ø±Û’ Ú©ÛŒ Ù¾ÛÙ„Û’ ÛÛŒ Ø¯Ùˆ Ø¨ÛŒÙˆÛŒØ§Úº ÛÛŒÛ’ Ú©ÛŒØ§ Ø§Ù¾ Ù†Û’ ØªÛŒØ³Ø±ÛŒ Ú©Û’ Ù„ÛŒÛ’ Ù¹Ø±Ø§Ø¦ Ú©Ø±Ù†Ø§ ÛÛ’ ğŸ˜œ\n",
            "Processed: Ø§Ø¨ Ø¨Ø³ Ø¨Ú¾ÛŒ Ú©Ø±Ùˆ Ø¨ÛŒÚ†Ø§Ø±Û’ Ù¾ÛÙ„Û’ ÛÛŒ Ø¯Ùˆ Ø¨ÛŒÙˆÛŒØ§Úº ÛÛŒÛ’ Ú©ÛŒØ§ Ø§Ù¾ ØªÛŒØ³Ø±ÛŒ Ù„ÛŒÛ’ Ù¹Ø±Ø§Ø¦ Ú©Ø±Ù†Ø§ ğŸ˜œ\n",
            "---\n",
            "Original: Ù¾ØªÛ Ù†ÛÛŒÚº Ú©ÛŒØ§ ÛÙˆØ±ÛØ§ ÛÛ’ Ø³ÛŒ Ø³ÛŒ Ú©ÛŒ Ø¨ÙˆØ±Úˆ Ú©Ùˆ Ø²Ù†Ú¯ Ù„Ú¯ Ú¯ÛŒØ§ ÛÛ’ğŸ˜­ğŸ˜­\n",
            "Processed: Ù¾ØªÛ Ù†ÛÛŒÚº Ú©ÛŒØ§ ÛÙˆØ±ÛØ§ Ø³ÛŒ Ø³ÛŒ Ø¨ÙˆØ±Úˆ Ø²Ù†Ú¯ Ù„Ú¯ Ú¯ÛŒØ§ ÛÛ’ğŸ˜­ğŸ˜­\n",
            "---\n",
            "Original: Ø§Ù„Ù„Û Ø¢Ù¾ Ú©Ùˆ Ø§Ù¾Ù†ÛŒ Ø±Ø­Ù…ØªÙˆÚº Ú©Û’ Ø³Ø§Ø¦Û’ Ù…ÛŒÚº Ø±Ú©Ú¾Û’ Ø§ÙˆØ± Ù…Ú©Ù…Ù„ ØµØ­Øª ÛŒØ§Ø¨ ÙØ±Ù…Ø§Ø¦Û’ ğŸ™\n",
            "Processed: Ø§Ù„Ù„Û Ø¢Ù¾ Ø§Ù¾Ù†ÛŒ Ø±Ø­Ù…ØªÙˆÚº Ø³Ø§Ø¦Û’ Ø±Ú©Ú¾Û’ Ù…Ú©Ù…Ù„ ØµØ­Øª ÛŒØ§Ø¨ ÙØ±Ù…Ø§Ø¦Û’ ğŸ™\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.15.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8jW7fd5cQNy",
        "outputId": "6fa1a178-f377-4a15-b396-27108a7b1351"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorstore 0.1.67 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BSNX_KzetLS",
        "outputId": "73dc0493-9f33-4c46-c02d-54e33a71169e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.4.0\n",
            "    Uninstalling typeguard-4.4.0:\n",
            "      Successfully uninstalled typeguard-4.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.4.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install urduhack nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL-XM55iewaR",
        "outputId": "93b2e19e-2440-485f-db3f-e9f7bd810b1b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting urduhack\n",
            "  Downloading urduhack-1.1.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting tf2crf (from urduhack)\n",
            "  Downloading tf2crf-0.1.33-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow-datasets~=3.1 (from urduhack)\n",
            "  Downloading tensorflow_datasets-3.2.1-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting Click~=7.1 (from urduhack)\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from urduhack) (2024.9.11)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.4.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (24.2.0)\n",
            "Collecting dill (from tensorflow-datasets~=3.1->urduhack)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.26.4)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (3.20.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.16.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (2.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.14.1)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tf2crf->urduhack) (2.15.0)\n",
            "Requirement already satisfied: tensorflow-addons>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from tf2crf->urduhack) (0.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (2024.8.30)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (24.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (4.12.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.15.0)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons>=0.8.2->tf2crf->urduhack) (2.13.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.1.0->tf2crf->urduhack) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (3.0.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (3.2.2)\n",
            "Downloading urduhack-1.1.1-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.5/105.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_datasets-3.2.1-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf2crf-0.1.33-py2.py3-none-any.whl (7.3 kB)\n",
            "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill, Click, tensorflow-datasets, tf2crf, urduhack\n",
            "  Attempting uninstall: Click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "  Attempting uninstall: tensorflow-datasets\n",
            "    Found existing installation: tensorflow-datasets 4.9.6\n",
            "    Uninstalling tensorflow-datasets-4.9.6:\n",
            "      Successfully uninstalled tensorflow-datasets-4.9.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 2.2.5 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "dask 2024.10.0 requires click>=8.1, but you have click 7.1.2 which is incompatible.\n",
            "typer 0.12.5 requires click>=8.0.0, but you have click 7.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Click-7.1.2 dill-0.3.9 tensorflow-datasets-3.2.1 tf2crf-0.1.33 urduhack-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from urduhack.normalization import normalize\n",
        "from nltk.stem import RSLPStemmer\n",
        "# Download the stemmer resources from nltk\n",
        "nltk.download('rslp')\n",
        "# Initialize the RSLP Stemmer from NLTK (works well for multiple languages)\n",
        "stemmer = RSLPStemmer()\n",
        "# Define a list of Urdu stopwords\n",
        "urdu_stopwords = [\n",
        "    \"Ø§ÙˆØ±\", \"ÛŒÛ\", \"Ú©Û\", \"ÛÛ’\", \"Ù…ÛŒÚº\", \"Ú©ÛŒ\", \"Ú©Ø§\", \"Ú©Û’\", \"Ú©Ùˆ\", \"Ù†Û’\", \"ÛÙˆÚº\",\n",
        "    \"ÛÛŒÚº\", \"ØªÚ¾Ø§\", \"ØªÚ¾ÛŒ\", \"ØªÚ¾Û’\", \"ÛÙˆ\", \"ÛÙˆØ§\", \"ÛÙˆØ¦\", \"ÛÙˆØ¦ÛŒ\", \"Ù¾Ø±\", \"Ú©Ø§\"\n",
        "]\n",
        "# Dictionary for lemmatization (using a more comprehensive example)\n",
        "lemma_dict = {\n",
        "    \"Ú†Ù„ Ø±ÛÛŒ\": \"Ú†Ù„\",\n",
        "    \"Ú†Ù„ØªÛ’\": \"Ú†Ù„\",\n",
        "    \"Ú†Ù„ØªØ§\": \"Ú†Ù„\",\n",
        "    \"Ø§Ú†Ú¾Ø§\": \"Ø§Ú†Ú¾Ø§\",\n",
        "    \"Ø§Ú†Ú¾ÛŒ\": \"Ø§Ú†Ú¾Ø§\",\n",
        "    \"Ø§Ú†Ú¾Û’\": \"Ø§Ú†Ú¾Ø§\",\n",
        "    \"Ú©Ø±ØªÛ’\": \"Ú©Ø±\",\n",
        "    \"Ú©Ø±ØªØ§\": \"Ú©Ø±\",\n",
        "    \"Ú©Ø±Ø±ÛÛ’\": \"Ú©Ø±\",\n",
        "    \"Ú¯Ø¦Û’\": \"Ø¬Ø§Ù†Ø§\",\n",
        "    \"Ú©Ø±Û’\": \"Ú©Ø±\",\n",
        "    \"Ø¯ÛŒÚ©Ú¾ØªÛ’\": \"Ø¯ÛŒÚ©Ú¾\",\n",
        "    \"Ø¯ÛŒÚ©Ú¾Ø§\": \"Ø¯ÛŒÚ©Ú¾\",\n",
        "    \"Ø¯ÛŒÚ©Ú¾\": \"Ø¯ÛŒÚ©Ú¾\",\n",
        "    \"Ø¬Ø§\": \"Ø¬Ø§Ù†Ø§\",\n",
        "    \"Ú©Ú¾Ø§\": \"Ú©Ú¾Ø§Ù†Ø§\",\n",
        "    \"Ù¾Ú‘Ú¾Ø§\": \"Ù¾Ú‘Ú¾Ù†Ø§\",\n",
        "    \"Ú†Ù„Ù†Ø§\": \"Ú†Ù„\",\n",
        "    \"Ø¨ÙˆÙ„Ø§\": \"Ø¨ÙˆÙ„\",\n",
        "    \"Ø³Ù†Ø§\": \"Ø³Ù†Ù†Ø§\"\n",
        "}\n",
        "def remove_stopwords(text, stopwords):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stopwords]\n",
        "    return \" \".join(filtered_words)\n",
        "def urdu_lemmatizer(word):\n",
        "    return lemma_dict.get(word, word)  # Use the lemma if available, otherwise return the original word\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Step 1: Normalize text using UrduHack\n",
        "    text = normalize(text)\n",
        "    # Step 2: Remove URLs, punctuation, and unwanted characters\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'#\\w+', '', text)     # Remove hashtags\n",
        "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
        "    # Step 3: Remove stopwords\n",
        "    processed_text = remove_stopwords(text, urdu_stopwords)\n",
        "    # Step 4: Stemming using NLTK RSLP Stemmer\n",
        "    stemmed_words = [stemmer.stem(word) for word in processed_text.split()]\n",
        "    stemmed_text = \" \".join(stemmed_words)\n",
        "    # Step 5: Lemmatization using custom dictionary\n",
        "    lemmatized_words = [urdu_lemmatizer(word) for word in stemmed_text.split()]\n",
        "    lemmatized_text = \" \".join(lemmatized_words)\n",
        "    return lemmatized_text\n",
        "# Open the file and read its content\n",
        "file_path = 'urdu_sarcastic_dataset.csv'\n",
        "with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    for i, row in enumerate(csv_reader):\n",
        "        if i >= 20:  # Process first 100 rows for testing (adjust as needed)\n",
        "            break\n",
        "        if row:\n",
        "            # Assuming the text is in the first column (index 0)\n",
        "            text = row[0]\n",
        "            preprocessed_text = preprocess_text(text)\n",
        "            print(f\"Original: {text}\")\n",
        "            print(f\"Preprocessed: {preprocessed_text}\")\n",
        "            print(\"---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYjM28sXWODV",
        "outputId": "1780d00e-f73d-4cca-fcc5-52ac7e9e14e6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: ï»¿urdu_text\n",
            "Preprocessed: urdu_text\n",
            "---\n",
            "Original: ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†Ø§ÛÛŒÛ’ ğŸ˜ğŸ˜ğŸ˜ğŸ¤£\n",
            "Preprocessed: Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†Ø§ÛÛŒÛ’\n",
            "---\n",
            "Original: Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚºğŸ˜‚ğŸ˜‚\n",
            "Preprocessed: Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº\n",
            "---\n",
            "Original: Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´Ù† Ú©ÛŒ Ú©Ø±Ø¯Ø§Ø± Ú©Ø´ÛŒ Ø§ÙˆØ±Ø§Ø³ Ù¾Ø±Ø¨Ú¾ÙˆÙ†Ú©Ù†Ø§ÛÛ’Ø¢Ù¾ Ø®ÙˆØ´Ø§Ù…Ø¯Ú¯Ø±ÛŒ ÙˆÚ†Ø§Ù¾Ù„ÙˆØ³ÛŒ Ø³Û’Ø§ÙˆØ±Ú©ØªÙ†ÛŒ Ø¯ÙˆÙ„Øª Ú©Ù…Ø§Ù†Ø§Ú†Ø§ÛØªÛ’ÛÛŒÚº Ù…ÙˆÙ¹Ø±Ø³Ø§Ø¦ÛŒÚ©Ù„ Ø³Û’Ù¾ÛŒØ¬Ø§Ø±Ùˆ Ù¾Ø±Ø§ÚˆÙˆ ØªÚ© Ú©Û’Ø³ÙØ±Ù…ÛŒÚº Ø¶Ù…ÛŒØ±Ú©ÛŒ Ù„Ø§Ø´ Ø³Û’Ø§Ù¹Ú¾ØªÛŒ Ø¨Ø¯Ø¨ÙˆØ¢Ù¾ Ú©ÛŒ Ù†Ø§Ú© Ø¨Ù†Ø¯ Ù†ÛÛŒÚº Ú©Ø±ØªÛŒ ÛÛ’ ğŸ™Ù†ÙˆÙ¹ Ø¢Ù¾ Ø³Ø¨ Ø³Û’Ø§Ù„ØªØ¬Ø§Ú¯Ø²Ø§Ø±Ø´ ÛÛ’ÛÙ…ÛŒÚº Ø¨Ú¾ÛŒ ÙØ§Ù„ÙˆÚ©Ø±ÛŒÚº Ø´Ú©Ø±ÛŒÛ\n",
            "Preprocessed: Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´Ù† Ú©Ø±Ø¯Ø§Ø± Ú©Ø´ÛŒ Ø§ÙˆØ±Ø§Ø³ Ù¾Ø±Ø¨Ú¾ÙˆÙ†Ú©Ù†Ø§ÛÛ’Ø¢Ù¾ Ø®ÙˆØ´Ø§Ù…Ø¯Ú¯Ø±ÛŒ ÙˆÚ†Ø§Ù¾Ù„ÙˆØ³ÛŒ Ø³Û’Ø§ÙˆØ±Ú©ØªÙ†ÛŒ Ø¯ÙˆÙ„Øª Ú©Ù…Ø§Ù†Ø§Ú†Ø§ÛØªÛ’ÛÛŒÚº Ù…ÙˆÙ¹Ø±Ø³Ø§Ø¦ÛŒÚ©Ù„ Ø³Û’Ù¾ÛŒØ¬Ø§Ø±Ùˆ Ù¾Ø±Ø§ÚˆÙˆ ØªÚ© Ú©Û’Ø³ÙØ±Ù…ÛŒÚº Ø¶Ù…ÛŒØ±Ú©ÛŒ Ù„Ø§Ø´ Ø³Û’Ø§Ù¹Ú¾ØªÛŒ Ø¨Ø¯Ø¨ÙˆØ¢Ù¾ Ù†Ø§Ú© Ø¨Ù†Ø¯ Ù†ÛÛŒÚº Ú©Ø±ØªÛŒ Ù†ÙˆÙ¹ Ø¢Ù¾ Ø³Ø¨ Ø³Û’Ø§Ù„ØªØ¬Ø§Ú¯Ø²Ø§Ø±Ø´ ÛÛ’ÛÙ…ÛŒÚº Ø¨Ú¾ÛŒ ÙØ§Ù„ÙˆÚ©Ø±ÛŒÚº Ø´Ú©Ø±ÛŒÛ\n",
            "---\n",
            "Original: Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜\n",
            "Preprocessed: Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ†\n",
            "---\n",
            "Original:  `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±ğŸ˜\n",
            "Preprocessed: Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±\n",
            "---\n",
            "Original:  Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº ğŸ’”ğŸ”¥\n",
            "Preprocessed: Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’\n",
            "---\n",
            "Original: Ø§Ù†Ø³Ø§Úº Ú©Ùˆ ØªÚ¾Ú©Ø§ Ø¯ÛŒØªØ§ ÛÛ’ Ø³ÙˆÚ†ÙˆÚº Ú©Ø§ Ø³ÙØ± Ø¨Ú¾ÛŒ ... ğŸğŸ¥€\n",
            "Preprocessed: Ø§Ù†Ø³Ø§Úº ØªÚ¾Ú©Ø§ Ø¯ÛŒØªØ§ Ø³ÙˆÚ†ÙˆÚº Ø³ÙØ± Ø¨Ú¾ÛŒ\n",
            "---\n",
            "Original: Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ØµØ§Ø­Ø¨ ÙˆÛŒÙ„ÚˆÙ†ğŸ‘ğŸ˜Š\n",
            "Preprocessed: Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ØµØ§Ø­Ø¨ ÙˆÛŒÙ„ÚˆÙ†\n",
            "---\n",
            "Original: ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ ÛÛ’ Ø§Ø³ Ø¢Ø±Û’ Ù„Ú¯Ø§ ÛÙˆÛŒØ§ ÛÛ’ğŸ˜‚ğŸ˜‚ ØªØ³ÛŒ ØªÛ’ Ù¾Ú©Û’ Ù†Ø¬ÙˆÙ…ÛŒ ÛÙˆ Ø§Ø³ÛŒ Ù…Ù†Ù†Ø¯Û’ ÛØ§Úº\n",
            "Preprocessed: ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ Ø§Ø³ Ø¢Ø±Û’ Ù„Ú¯Ø§ ÛÙˆÛŒØ§ ØªØ³ÛŒ ØªÛ’ Ù¾Ú©Û’ Ù†Ø¬ÙˆÙ…ÛŒ Ø§Ø³ÛŒ Ù…Ù†Ù†Ø¯Û’ ÛØ§Úº\n",
            "---\n",
            "Original: ÛŒÛ Ø³Ù…Ø¬Ú¾ØªÛ’ ÛÛŒÚº Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’ ğŸ˜‚ğŸ˜‚ğŸ˜‚\n",
            "Preprocessed: Ø³Ù…Ø¬Ú¾ØªÛ’ Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’\n",
            "---\n",
            "Original: ØªØ³ÛŒ Ù„Ú‘Ø§ÚºÙ”ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§ÚˆÛŒ Ú©ÛŒ ğŸ˜‚ğŸ˜‚ğŸ˜‚\n",
            "Preprocessed: ØªØ³ÛŒ Ù„Ú‘Ø§ÚºÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§ÚˆÛŒ\n",
            "---\n",
            "Original: Ù¾Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’..ØŸØŸğŸ˜†ğŸ™„\n",
            "Preprocessed: Ù¾Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’\n",
            "---\n",
            "Original: Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ ÛÛ’ Ø§Ù„Ùˆ Ø¯ÙˆØ³Ùˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ 80Ø±ÙˆÙ¾Û’ Ú¯Ø² ğŸ˜­ğŸ˜­ğŸ˜­ ISPR\n",
            "Preprocessed: Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ Ø§Ù„Ùˆ Ø¯ÙˆØ³Ùˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ 80Ø±ÙˆÙ¾Û’ Ú¯Ø² ispr\n",
            "---\n",
            "Original: ğŸ˜Ø¹Ø´Ù‚ Ø¬Ø¨ ØªÙ… Ú©Ùˆ Ø±Ø§Ø³ Ø¢Û“ Ú¯Ø§ ğŸ’”Ø²Ø®Ù… Ú©Ú¾Ø§Ù¶ Ú¯Û’ ğŸ˜ŠÙ…ÙØ³Ú©Ø±Ø§Ù¶ Ú¯Û’\n",
            "Preprocessed: Ø¹Ø´Ù‚ Ø¬Ø¨ ØªÙ… Ø±Ø§Ø³ Ø¢Û“ Ú¯Ø§ Ø²Ø®Ù… Ú©Ú¾Ø§Ù¶ Ú¯Û’ Ù…Ø³Ú©Ø±Ø§Ù¶ Ú¯Û’\n",
            "---\n",
            "Original: Ú†ÙˆÙ†Ø§ Ø§ÛŒØ³Ø§ ÛÛŒ ÛÙˆØªØ§ ğŸ˜‚\n",
            "Preprocessed: Ú†ÙˆÙ†Ø§ Ø§ÛŒØ³Ø§ ÛÛŒ ÛÙˆØªØ§\n",
            "---\n",
            "Original: Ø®Ø§ØªÙ…_Ø§Ù„Ù†Ø¨ÛŒÛŒÙ†_Ù…Ø­Ù…Ø¯ï·º Surat 73 Ø³ÙˆØ±Ø© Ø§Ù„Ù…Ø²Ù…Ù„ Ayt 20 Ø­ØµÛ5 ØªØ±Ø¬Ù…Û Ø§ÙˆØ± Ú©Ú†Ú¾ Ø§Ù„Ù„Û Ú©ÛŒ Ø±Ø§Û Ù…ÛŒÚº Ù„Ú‘ØªÛ’ ÛÙˆÚº Ú¯Û’ ØªÙˆ Ø¬ØªÙ†Ø§ Ù‚Ø±Ø¢Ù† Ù…ÛŒØ³Ø± ÛÙˆ Ù¾Ú‘Ú¾ÙˆØ§ÙˆØ±Ù†Ù…Ø§Ø² Ù‚Ø§Ø¦Ù… Ø±Ú©Ú¾ÙˆØ§ÙˆØ±Ø²Ú©ÙˆÙ°Ø© Ø¯ÙˆØ§ÙˆØ± Ø§Ù„Ù„Û Ú©Ùˆ Ø§Ú†Ú¾Ø§ Ù‚Ø±Ø¶ Ø¯Ùˆ ğŸŒ¹ğŸŒ¹ Ø¯Ø±ÙˆØ¯_ÙˆÙ‚Ø±Ø¢Ù† ØµÙÙ„ÙÙ‘ÛŒ Ø§Ù„Ù„ÛÙ Ø¹ÙÙ„Ù°ÛŒ Ø­ÙØ¨ÙÛŒÙ’Ø¨ÙÛÙ– Ø³ÛŒÙÙ‘Ø¯ÙÙ†ÙØ§ Ù…ÙØ­ÙÙ…ÙÙ‘Ø¯Ù ÙˆÙÙ‘Ø¢Ù„ÙÛÙ– ÙˆÙØµÙØ­Ù’Ø¨ÙÛÙ– ÙˆÙØ³ÙÙ„ÙÙ‘Ù…Ù’\n",
            "Preprocessed: Ø®Ø§ØªÙ…_Ø§Ù„Ù†Ø¨ÛŒÛŒÙ†_Ù…Ø­Ù…Ø¯ï·º surat 73 Ø³ÙˆØ±Ûƒ Ø§Ù„Ù…Ø²Ù…Ù„ ayt 20 Ø­ØµÛ5 ØªØ±Ø¬Ù…Û Ú©Ú†Ú¾ Ø§Ù„Ù„Û Ø±Ø§Û Ù„Ú‘ØªÛ’ Ú¯Û’ ØªÙˆ Ø¬ØªÙ†Ø§ Ù‚Ø±Ø¢Ù† Ù…ÛŒØ³Ø± Ù¾Ú‘Ú¾ÙˆØ§ÙˆØ±Ù†Ù…Ø§Ø² Ù‚Ø§Ø¦Ù… Ø±Ú©Ú¾ÙˆØ§ÙˆØ±Ø²Ú©ÙˆÛƒ Ø¯ÙˆØ§ÙˆØ± Ø§Ù„Ù„Û Ø§Ú†Ú¾Ø§ Ù‚Ø±Ø¶ Ø¯Ùˆ Ø¯Ø±ÙˆØ¯_ÙˆÙ‚Ø±Ø¢Ù† ØµÙ„ÛŒ Ø§Ù„Ù„Û Ø¹Ù„ÛŒ Ø­Ø¨ÛŒØ¨Û Ø³ÛŒØ¯Ù†Ø§ Ù…Ø­Ù…Ø¯ ÙˆØ¢Ù„Û ÙˆØµØ­Ø¨Û ÙˆØ³Ù„Ù…\n",
            "---\n",
            "Original: Ø§Ø¨ Ø¨Ø³ Ø¨Ú¾ÛŒ Ú©Ø±Ùˆ Ø¨ÛŒÚ†Ø§Ø±Û’ Ú©ÛŒ Ù¾ÛÙ„Û’ ÛÛŒ Ø¯Ùˆ Ø¨ÛŒÙˆÛŒØ§Úº ÛÛŒÛ’ Ú©ÛŒØ§ Ø§Ù¾ Ù†Û’ ØªÛŒØ³Ø±ÛŒ Ú©Û’ Ù„ÛŒÛ’ Ù¹Ø±Ø§Ø¦ Ú©Ø±Ù†Ø§ ÛÛ’ ğŸ˜œ\n",
            "Preprocessed: Ø§Ø¨ Ø¨Ø³ Ø¨Ú¾ÛŒ Ú©Ø±Ùˆ Ø¨ÛŒÚ†Ø§Ø±Û’ Ù¾ÛÙ„Û’ ÛÛŒ Ø¯Ùˆ Ø¨ÛŒÙˆÛŒØ§Úº ÛÛŒÛ’ Ú©ÛŒØ§ Ø§Ù¾ ØªÛŒØ³Ø±ÛŒ Ù„ÛŒÛ’ Ù¹Ø±Ø§Ø¦ Ú©Ø±Ù†Ø§\n",
            "---\n",
            "Original: Ù¾ØªÛ Ù†ÛÛŒÚº Ú©ÛŒØ§ ÛÙˆØ±ÛØ§ ÛÛ’ Ø³ÛŒ Ø³ÛŒ Ú©ÛŒ Ø¨ÙˆØ±Úˆ Ú©Ùˆ Ø²Ù†Ú¯ Ù„Ú¯ Ú¯ÛŒØ§ ÛÛ’ğŸ˜­ğŸ˜­\n",
            "Preprocessed: Ù¾ØªÛ Ù†ÛÛŒÚº Ú©ÛŒØ§ ÛÙˆØ±ÛØ§ Ø³ÛŒ Ø³ÛŒ Ø¨ÙˆØ±Úˆ Ø²Ù†Ú¯ Ù„Ú¯ Ú¯ÛŒØ§\n",
            "---\n",
            "Original: Ø§Ù„Ù„Û Ø¢Ù¾ Ú©Ùˆ Ø§Ù¾Ù†ÛŒ Ø±Ø­Ù…ØªÙˆÚº Ú©Û’ Ø³Ø§Ø¦Û’ Ù…ÛŒÚº Ø±Ú©Ú¾Û’ Ø§ÙˆØ± Ù…Ú©Ù…Ù„ ØµØ­Øª ÛŒØ§Ø¨ ÙØ±Ù…Ø§Ø¦Û’ ğŸ™\n",
            "Preprocessed: Ø§Ù„Ù„Û Ø¢Ù¾ Ø§Ù¾Ù†ÛŒ Ø±Ø­Ù…ØªÙˆÚº Ø³Ø§Ø¦Û’ Ø±Ú©Ú¾Û’ Ù…Ú©Ù…Ù„ ØµØ­Øª ÛŒØ§Ø¨ ÙØ±Ù…Ø§Ø¦Û’\n",
            "---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk scikit-learn gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5CDblzfWY3e",
        "outputId": "e12898ef-3ad3-4513-e720-d1e496259de5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure required NLTK data is available for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to preprocess the text (tokenization)\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the text by tokenizing it.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be tokenized.\n",
        "\n",
        "    Returns:\n",
        "        list: List of tokens.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text)  # Tokenize the text using NLTK's word_tokenize\n",
        "    return tokens\n",
        "\n",
        "# Open the CSV file and read its content using Python's built-in csv module\n",
        "file_path = 'urdu_sarcastic_dataset.csv'  # Specify the path to the CSV file\n",
        "preprocessed_texts = []  # List to store preprocessed text data\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "\n",
        "    # Iterate over each row in the CSV\n",
        "    for i, row in enumerate(csv_reader):\n",
        "        if i >= 5500:  # Process a limited number of rows (adjust as needed)\n",
        "            break\n",
        "\n",
        "        # Check if the row contains valid text data\n",
        "        if row and isinstance(row[0], str):\n",
        "            text = row[0]  # Assume the text is in the first column\n",
        "            # Preprocess the text and append it to the list\n",
        "            preprocessed_texts.append(\" \".join(preprocess_text(text)))\n",
        "\n",
        "# Tokenization for Word2Vec training (re-tokenize the preprocessed texts)\n",
        "tokenized_texts = [preprocess_text(text) for text in preprocessed_texts]\n",
        "\n",
        "# TF-IDF Feature Extraction\n",
        "vectorizer = TfidfVectorizer()  # Initialize the TF-IDF vectorizer\n",
        "tfidf_matrix = vectorizer.fit_transform(preprocessed_texts)  # Fit and transform the text data\n",
        "\n",
        "# Get feature names (words) from the TF-IDF model\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix to a dense format for easier manipulation\n",
        "dense = tfidf_matrix.todense()\n",
        "denselist = dense.tolist()\n",
        "\n",
        "# Get the top 10 words with highest TF-IDF scores across the dataset\n",
        "tfidf_scores = [(feature_names[col], score) for row in denselist for col, score in enumerate(row)]\n",
        "tfidf_scores.sort(key=lambda x: x[1], reverse=True)  # Sort the words by their TF-IDF scores\n",
        "top_10_tfidf_words = tfidf_scores[:10]  # Get the top 10 words\n",
        "\n",
        "# Print the top 10 words based on TF-IDF scores\n",
        "print(\"Top 10 TF-IDF words:\")\n",
        "for word, score in top_10_tfidf_words:\n",
        "    print(f\"{word}: {score}\")\n",
        "\n",
        "# Word2Vec Training\n",
        "# Initialize and train the Word2Vec model with tokenized text\n",
        "model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=20, min_count=1, workers=7)\n",
        "model.train(tokenized_texts, total_examples=len(tokenized_texts), epochs=10)  # Train the model\n",
        "\n",
        "# Find the 5 words most similar to \"Ø§Ú†Ú¾Ø§\" (good)\n",
        "try:\n",
        "    similar_words = model.wv.most_similar(\"Ø§Ú†Ú¾Ø§\", topn=5)  # Find the top 5 most similar words\n",
        "    print(\"\\n5 words closest to 'Ø§Ú†Ú¾Ø§':\")\n",
        "    for word, similarity in similar_words:\n",
        "        print(f\"{word}: {similarity}\")\n",
        "\n",
        "except KeyError:\n",
        "    print(\"\\n'Ø§Ú†Ú¾Ø§' not found in the vocabulary.\")  # Handle case where the word is not in the model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvkdH4oze1iK",
        "outputId": "c223c92a-2795-4cfe-dcfd-e6ce0bdd39e1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 TF-IDF words:\n",
            "urdu_text: 1.0\n",
            "ÚˆØ±Ø§Ù…ÛŒ: 1.0\n",
            "Ø§Ø³ØªØºÙØ±Ø§Ù„Ù„Û: 1.0\n",
            "Ø§Ú©Ø§Ø¤Ù†Ù¹: 1.0\n",
            "Ø§Ú¾Ø§Úº: 1.0\n",
            "Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Û: 1.0\n",
            "ÛØ§ÛØ§ÛØ§: 1.0\n",
            "Ø§ÛÙˆ: 1.0\n",
            "Ø¢ÛÙˆ: 1.0\n",
            "ØªÙˆØ¨Û: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5 words closest to 'Ø§Ú†Ú¾Ø§':\n",
            "Ø§ØªÙ†Ø§: 0.9929912686347961\n",
            "ÙˆÛŒØ³Û’: 0.9843923449516296\n",
            "Ø§ÛŒØ³Ø§: 0.9698293805122375\n",
            "ØªÙ…ÛÛŒÚº: 0.9675552248954773\n",
            "Ú©ÛŒÚ‘Û’: 0.9665562510490417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "# Ensure required NLTK data is available for tokenization\n",
        "nltk.download('punkt')\n",
        "def preprocess_text(text):\n",
        "    return nltk.word_tokenize(text)\n",
        "def generate_ngrams(tokens, n):\n",
        "    return list(ngrams(tokens, n))\n",
        "def read_csv(file_path):\n",
        "    texts = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file)\n",
        "        # Iterate through each row in the CSV\n",
        "        for i, row in enumerate(csv_reader):\n",
        "            if i >= 20:  # Limit the number of rows to process (adjust as necessary)\n",
        "                break\n",
        "            # Ensure the row contains valid text data\n",
        "            if row and isinstance(row[0], str):\n",
        "                text = row[0]  # Assuming the text is in the first column\n",
        "                texts.append(text)\n",
        "    return texts\n",
        "def compute_ngram_frequencies(texts, n):\n",
        "    all_ngrams = []\n",
        "    # Iterate through each text, tokenize it, and generate n-grams\n",
        "    for text in texts:\n",
        "        tokens = preprocess_text(text)\n",
        "        ngrams_list = generate_ngrams(tokens, n)\n",
        "        all_ngrams.extend(ngrams_list)  # Add n-grams to the collection\n",
        "    return Counter(all_ngrams)  # Count frequencies of each n-gram\n",
        "def print_top_ngrams(ngram_counts, n=10):\n",
        "    top_ngrams = ngram_counts.most_common(n)  # Get the top N most common n-grams\n",
        "    for ngram, freq in top_ngrams:\n",
        "        print(f\"{ngram}: {freq}\")  # Print each n-gram and its frequency\n",
        "if __name__ == \"__main__\":\n",
        "    # File path to the Urdu text CSV\n",
        "    file_path = 'urdu_sarcastic_dataset.csv'  # Specify the path to your CSV file\n",
        "    # Read the CSV and get the texts (up to 5500 rows)\n",
        "    texts = read_csv(file_path)\n",
        "    # Compute bigram and trigram frequencies\n",
        "    bigram_counts = compute_ngram_frequencies(texts, 2)  # Compute frequencies for bigrams\n",
        "    trigram_counts = compute_ngram_frequencies(texts, 3)  # Compute frequencies for trigrams\n",
        "    # Display the top 10 bigrams\n",
        "    print(\"Top 10 Bigrams:\")\n",
        "    print_top_ngrams(bigram_counts)\n",
        "    # Display the top 10 trigrams\n",
        "    print(\"\\nTop 10 Trigrams:\")\n",
        "    print_top_ngrams(trigram_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNMT3g6FVX6W",
        "outputId": "4db2cdad-a2f6-48ce-db8d-5ac8f26c859f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Bigrams:\n",
            "('ğŸ¤£ğŸ˜‚ğŸ˜‚', 'ÛÙˆ'): 1\n",
            "('ÛÙˆ', 'Ù„ÛŒÙ†Û’'): 1\n",
            "('Ù„ÛŒÙ†Û’', 'Ø¯Û’'): 1\n",
            "('Ø¯Û’', 'Ù…ÛŒØ±ÛŒ'): 1\n",
            "('Ù…ÛŒØ±ÛŒ', 'Ø´Ø§Ø¯ÛŒ'): 1\n",
            "('Ø´Ø§Ø¯ÛŒ', 'ÙØ³Ø§Ø¯Ù†'): 1\n",
            "('ÙØ³Ø§Ø¯Ù†', 'Ù¹Ú¾ÛŒÚ©'): 1\n",
            "('Ù¹Ú¾ÛŒÚ©', 'ÛÛ’'): 1\n",
            "('ÛÛ’', 'Ú©ÙˆØ¬ÛŒ'): 1\n",
            "('Ú©ÙˆØ¬ÛŒ', 'Ù†ÛÛŒÚº'): 1\n",
            "\n",
            "Top 10 Trigrams:\n",
            "('ğŸ¤£ğŸ˜‚ğŸ˜‚', 'ÛÙˆ', 'Ù„ÛŒÙ†Û’'): 1\n",
            "('ÛÙˆ', 'Ù„ÛŒÙ†Û’', 'Ø¯Û’'): 1\n",
            "('Ù„ÛŒÙ†Û’', 'Ø¯Û’', 'Ù…ÛŒØ±ÛŒ'): 1\n",
            "('Ø¯Û’', 'Ù…ÛŒØ±ÛŒ', 'Ø´Ø§Ø¯ÛŒ'): 1\n",
            "('Ù…ÛŒØ±ÛŒ', 'Ø´Ø§Ø¯ÛŒ', 'ÙØ³Ø§Ø¯Ù†'): 1\n",
            "('Ø´Ø§Ø¯ÛŒ', 'ÙØ³Ø§Ø¯Ù†', 'Ù¹Ú¾ÛŒÚ©'): 1\n",
            "('ÙØ³Ø§Ø¯Ù†', 'Ù¹Ú¾ÛŒÚ©', 'ÛÛ’'): 1\n",
            "('Ù¹Ú¾ÛŒÚ©', 'ÛÛ’', 'Ú©ÙˆØ¬ÛŒ'): 1\n",
            "('ÛÛ’', 'Ú©ÙˆØ¬ÛŒ', 'Ù†ÛÛŒÚº'): 1\n",
            "('Ú©ÙˆØ¬ÛŒ', 'Ù†ÛÛŒÚº', 'Ú†Ø§ÛÛŒÛ’'): 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "# Ensure required NLTK data is available for tokenization\n",
        "nltk.download('punkt')\n",
        "# Function to preprocess the text (tokenization)\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "# Path to the CSV file containing the Urdu text\n",
        "file_path = 'urdu_sarcastic_dataset.csv'  # Change this to your actual file path\n",
        "preprocessed_texts = []  # List to store preprocessed texts\n",
        "# Read the CSV file and preprocess each row\n",
        "with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    # Iterate over each row in the CSV file\n",
        "    for i, row in enumerate(csv_reader):\n",
        "        if i >= 20:  # Limit the number of rows to process (adjust as necessary)\n",
        "            break\n",
        "        if row and isinstance(row[0], str):  # Ensure the row contains valid text\n",
        "            text = row[0]  # Assuming the text is in the first column\n",
        "            preprocessed_texts.append(\" \".join(preprocess_text(text)))  # Tokenize and join tokens back into a string\n",
        "# Assuming you have a list of sentiment labels (0 for negative, 1 for positive)\n",
        "labels = np.random.randint(2, size=len(preprocessed_texts))  # Random labels, replace with actual labels from dataset\n",
        "# Option 1: TF-IDF Feature Extraction\n",
        "vectorizer = TfidfVectorizer()  # Initialize the TF-IDF vectorizer\n",
        "tfidf_matrix = vectorizer.fit_transform(preprocessed_texts)  # Fit and transform the preprocessed texts\n",
        "# Option 2: Word2Vec Feature Extraction\n",
        "tokenized_texts = [preprocess_text(text) for text in preprocessed_texts]  # Re-tokenize the preprocessed texts\n",
        "word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)  # Train Word2Vec model\n",
        "# Calculate the average word vector for each text\n",
        "X_word2vec = np.array([np.mean([word2vec_model.wv[word] for word in words if word in word2vec_model.wv] or [np.zeros(100)], axis=0) for words in tokenized_texts])\n",
        "# Use TF-IDF matrix for the model (you can switch to X_word2vec for Word2Vec features)\n",
        "X = tfidf_matrix  # Use TF-IDF features (or replace with X_word2vec for Word2Vec features)\n",
        "y = labels  # Target labels\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Initialize the Logistic Regression model\n",
        "classifier = LogisticRegression(max_iter=1000)  # Set max_iter to ensure convergence\n",
        "# Train the model on the training set\n",
        "classifier.fit(X_train, y_train)\n",
        "# Make predictions on the test set\n",
        "y_pred = classifier.predict(X_test)\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy\n",
        "precision = precision_score(y_test, y_pred, average='weighted')  # Calculate precision\n",
        "recall = recall_score(y_test, y_pred, average='weighted')  # Calculate recall\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')  # Calculate F1-score\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_zHUO2_bwB0",
        "outputId": "4310955b-ebf5-40d2-fd2d-f84644be1e19"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2500\n",
            "Precision: 0.0625\n",
            "Recall: 0.2500\n",
            "F1-Score: 0.1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Assuming X (features) and y (labels) are already prepared (e.g., from TF-IDF or Word2Vec features and sentiment labels)\n",
        "# Splitting the data into training and validation sets (80% training, 20% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Initialize the logistic regression classifier and train it on the training set\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train, y_train)\n",
        "# Make predictions on the validation set\n",
        "y_val_pred = classifier.predict(X_val)\n",
        "# Calculate evaluation metrics on the validation set\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)  # Calculate accuracy\n",
        "val_precision = precision_score(y_val, y_val_pred, average='weighted')  # Calculate precision (weighted average)\n",
        "val_recall = recall_score(y_val, y_val_pred, average='weighted')  # Calculate recall (weighted average)\n",
        "val_f1 = f1_score(y_val, y_val_pred, average='weighted')  # Calculate F1-score (weighted average)\n",
        "# Print the validation metrics\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"Validation Precision: {val_precision:.4f}\")\n",
        "print(f\"Validation Recall: {val_recall:.4f}\")\n",
        "print(f\"Validation F1-Score: {val_f1:.4f}\")\n",
        "# Generate the confusion matrix to evaluate classification performance\n",
        "conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
        "# Plot the confusion matrix using seaborn for better visualization\n",
        "plt.figure(figsize=(8, 6))  # Set figure size\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])  # Create heatmap\n",
        "plt.title('Confusion Matrix')  # Set plot title\n",
        "plt.xlabel('Predicted Label')  # Label x-axis as Predicted Label\n",
        "plt.ylabel('True Label')  # Label y-axis as True Label\n",
        "plt.show()  # Display the confusion matrix\n",
        "# Generate a detailed classification report to evaluate precision, recall, and F1-score for each class\n",
        "report = classification_report(y_val, y_val_pred, target_names=['Negative', 'Positive'])\n",
        "print(\"Classification Report:\\n\", report)\n",
        "# Identify and print the first 5 misclassified examples for further analysis\n",
        "print(\"\\nMisclassified Examples:\")\n",
        "misclassified_indices = np.where(y_val != y_val_pred)[0]  # Find indices where predictions and true labels don't match\n",
        "for index in misclassified_indices[:5]:  # Limit to first 5 misclassified examples\n",
        "    print(f\"Example {index}:\")\n",
        "    print(f\"Text: {preprocessed_texts[index]}\")  # Print the text of the misclassified example\n",
        "    print(f\"Actual Label: {y_val[index]}, Predicted Label: {y_val_pred[index]}\")  # Print the actual and predicted labels\n",
        "    print(\"---\")\n",
        "# Identify patterns or challenges in the misclassified examples\n",
        "print(\"\\nPotential Challenges in Urdu Sentiment Analysis based on misclassified examples:\")\n",
        "for index in misclassified_indices[:5]:  # Limit to first 5 examples for readability\n",
        "    text = preprocessed_texts[index]  # Get the misclassified text\n",
        "    actual_label = y_val[index]  # Get the actual label\n",
        "    predicted_label = y_val_pred[index]  # Get the predicted label\n",
        "    if predicted_label != actual_label:  # Analyze only misclassified cases\n",
        "        print(f\"Example {index} shows a challenge with:\")\n",
        "        # Check for complex sentence structure (example logic: if sentence is longer than 15 words)\n",
        "        if len(text.split()) > 15:\n",
        "            print(\"  - Complex sentence structure\")\n",
        "        # Check for possible sarcasm or ambiguous sentiment (example logic: check for exclamation or question marks)\n",
        "        if \"!\" in text or \"ØŸ\" in text:  # \"ØŸ\" is a common Urdu question mark\n",
        "            print(\"  - Possible sarcasm or ambiguous sentiment\")\n",
        "        # Detect colloquial language or slang (example logic: check for specific Urdu slang words)\n",
        "        if any(word in text for word in ['ÛŒØ§Ø±', 'Ù„ÙˆÚ¯', 'Ú©ÛŒØ§']):  # Example Urdu slang/colloquial words\n",
        "            print(\"  - Colloquial language or slang\")\n",
        "        print(\"---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OwkvVlg_ddIf",
        "outputId": "5c9df6b5-51e0-4dea-81fa-85c86b6b68de"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.2500\n",
            "Validation Precision: 0.0625\n",
            "Validation Recall: 0.2500\n",
            "Validation F1-Score: 0.1000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00         3\n",
            "    Positive       0.25      1.00      0.40         1\n",
            "\n",
            "    accuracy                           0.25         4\n",
            "   macro avg       0.12      0.50      0.20         4\n",
            "weighted avg       0.06      0.25      0.10         4\n",
            "\n",
            "\n",
            "Misclassified Examples:\n",
            "Example 1:\n",
            "Text: ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†Ø§ÛÛŒÛ’ ğŸ˜ğŸ˜ğŸ˜ğŸ¤£\n",
            "Actual Label: 0, Predicted Label: 1\n",
            "---\n",
            "Example 2:\n",
            "Text: Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚºğŸ˜‚ğŸ˜‚\n",
            "Actual Label: 0, Predicted Label: 1\n",
            "---\n",
            "Example 3:\n",
            "Text: Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´Ù† Ú©ÛŒ Ú©Ø±Ø¯Ø§Ø± Ú©Ø´ÛŒ Ø§ÙˆØ±Ø§Ø³ Ù¾Ø±Ø¨Ú¾ÙˆÙ†Ú©Ù†Ø§ÛÛ’Ø¢Ù¾ Ø®ÙˆØ´Ø§Ù…Ø¯Ú¯Ø±ÛŒ ÙˆÚ†Ø§Ù¾Ù„ÙˆØ³ÛŒ Ø³Û’Ø§ÙˆØ±Ú©ØªÙ†ÛŒ Ø¯ÙˆÙ„Øª Ú©Ù…Ø§Ù†Ø§Ú†Ø§ÛØªÛ’ÛÛŒÚº Ù…ÙˆÙ¹Ø±Ø³Ø§Ø¦ÛŒÚ©Ù„ Ø³Û’Ù¾ÛŒØ¬Ø§Ø±Ùˆ Ù¾Ø±Ø§ÚˆÙˆ ØªÚ© Ú©Û’Ø³ÙØ±Ù…ÛŒÚº Ø¶Ù…ÛŒØ±Ú©ÛŒ Ù„Ø§Ø´ Ø³Û’Ø§Ù¹Ú¾ØªÛŒ Ø¨Ø¯Ø¨ÙˆØ¢Ù¾ Ú©ÛŒ Ù†Ø§Ú© Ø¨Ù†Ø¯ Ù†ÛÛŒÚº Ú©Ø±ØªÛŒ ÛÛ’ ğŸ™Ù†ÙˆÙ¹ Ø¢Ù¾ Ø³Ø¨ Ø³Û’Ø§Ù„ØªØ¬Ø§Ú¯Ø²Ø§Ø±Ø´ ÛÛ’ÛÙ…ÛŒÚº Ø¨Ú¾ÛŒ ÙØ§Ù„ÙˆÚ©Ø±ÛŒÚº Ø´Ú©Ø±ÛŒÛ\n",
            "Actual Label: 0, Predicted Label: 1\n",
            "---\n",
            "\n",
            "Potential Challenges in Urdu Sentiment Analysis based on misclassified examples:\n",
            "Example 1 shows a challenge with:\n",
            "---\n",
            "Example 2 shows a challenge with:\n",
            "---\n",
            "Example 3 shows a challenge with:\n",
            "  - Complex sentence structure\n",
            "---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}