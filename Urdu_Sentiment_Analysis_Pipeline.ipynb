{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMaZyzQOPZXs",
        "outputId": "95059f66-7435-4287-bd1b-1d55afab522a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffurdu_text', 'is_sarcastic', '', '', '', '', '', '']\n",
            "['🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہیں چاہیے 😐😐😐🤣', '1', '', '', '', '', '', '']\n",
            "['چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی آں میں😂😂', '1', '', '', '', '', '', '']\n",
            "['کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپوزیشن کی کردار کشی اوراس پربھونکناہےآپ خوشامدگری وچاپلوسی سےاورکتنی دولت کماناچاہتےہیں موٹرسائیکل سےپیجارو پراڈو تک کےسفرمیں ضمیرکی لاش سےاٹھتی بدبوآپ کی ناک بند نہیں کرتی ہے 🙏نوٹ آپ سب سےالتجاگزارش ہےہمیں بھی فالوکریں شکریہ', '0', '', '', '', '', '', '']\n",
            "['نہیں پائین 😎', '0', '', '', '', '', '', '']\n",
            "[\" `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی تھے '' حامد میر😁\", '1', '', '', '', '', '', '']\n",
            "[' قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥', '1', '', '', '', '', '', '']\n",
            "['انساں کو تھکا دیتا ہے سوچوں کا سفر بھی ... 🍁🥀', '0', '', '', '', '', '', '']\n",
            "['حامد میر صاحب ویلڈن👏😊', '0', '', '', '', '', '', '']\n",
            "['یار وچارہ ویلا ہوندا ہے اس آرے لگا ہویا ہے😂😂 تسی تے پکے نجومی ہو اسی منندے ہاں', '1', '', '', '', '', '', '']\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "file_path = 'urdu_sarcastic_dataset.csv'  # Update this with the actual path to your CSV file\n",
        "\n",
        "# Open the CSV file in read mode\n",
        "with open(file_path, 'r') as csv_file:\n",
        "    # Create a CSV reader object to read the file\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "\n",
        "    # Loop through the first 10 rows and print each row\n",
        "    for i, row in enumerate(csv_reader):\n",
        "        # If the index reaches 10, exit the loop\n",
        "        if i >= 10:\n",
        "            break\n",
        "        # Print the current row\n",
        "        print(row)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a list of Urdu stopwords (can be extended based on the dataset)\n",
        "urdu_stopwords = [\n",
        "    \"اور\", \"یہ\", \"کہ\", \"ہے\", \"میں\", \"کی\", \"کا\", \"کے\", \"کو\", \"نے\", \"ہوں\", \"ہیں\",\n",
        "    \"تھا\", \"تھی\", \"تھے\", \"ہو\", \"ہوا\", \"ہوئ\", \"ہوئی\", \"پر\", \"سے\", \"کے\", \"ہم\", \"تم\"\n",
        "]\n",
        "# Example words that may carry sentiment but are commonly considered stopwords\n",
        "sentiment_stopwords = [\"نہیں\", \"برا\", \"اچھا\", \"غلط\"]\n",
        "def remove_stopwords(text, stopwords, sentiment_words=None):\n",
        "    words = text.split()  # Split text into individual words\n",
        "    filtered_words = [\n",
        "        word for word in words\n",
        "        if word not in stopwords or (sentiment_words and word in sentiment_words)\n",
        "    ]  # Remove stopwords but keep sentiment words\n",
        "    return \" \".join(filtered_words)  # Join words back into a string\n",
        "def process_file(file_path, stopwords, sentiment_words=None, encoding='utf-8'):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding=encoding) as csv_file:\n",
        "            csv_reader = csv.reader(csv_file)\n",
        "            # Process a limited number of rows, adjust as necessary\n",
        "            for i, row in enumerate(csv_reader):\n",
        "                if i >= 20:  # Adjust this limit based on dataset size\n",
        "                    break\n",
        "                if row:  # Ensure the row is not empty\n",
        "                    text = row[0]  # Assuming text is in the first column\n",
        "                    processed_text = remove_stopwords(text, stopwords, sentiment_words)\n",
        "\n",
        "                    # Display original and processed text\n",
        "                    print(f\"Original: {text}\")\n",
        "                    print(f\"Processed: {processed_text}\")\n",
        "                    print(\"---\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File {file_path} not found.\")\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"Error: Encoding issue with {file_path}. Try adjusting the encoding.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "# Main function to run the file processing\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to the CSV file\n",
        "    file_path = 'urdu_sarcastic_dataset.csv'\n",
        "    # Start processing the file\n",
        "    process_file(file_path, urdu_stopwords, sentiment_words=sentiment_stopwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SebhPQ66VicA",
        "outputId": "564224f5-882a-4c86-e9c6-0729646bc017"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: ﻿urdu_text\n",
            "Processed: ﻿urdu_text\n",
            "---\n",
            "Original: 🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہیں چاہیے 😐😐😐🤣\n",
            "Processed: 🤣😂😂 لینے دے میری شادی فسادن ٹھیک کوجی نہیں چاہیے 😐😐😐🤣\n",
            "---\n",
            "Original: چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی آں میں😂😂\n",
            "Processed: چل مہمانوں کھانا سرو کر چڑیل چاچی نوں دسدی آں میں😂😂\n",
            "---\n",
            "Original: کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپوزیشن کی کردار کشی اوراس پربھونکناہےآپ خوشامدگری وچاپلوسی سےاورکتنی دولت کماناچاہتےہیں موٹرسائیکل سےپیجارو پراڈو تک کےسفرمیں ضمیرکی لاش سےاٹھتی بدبوآپ کی ناک بند نہیں کرتی ہے 🙏نوٹ آپ سب سےالتجاگزارش ہےہمیں بھی فالوکریں شکریہ\n",
            "Processed: کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپوزیشن کردار کشی اوراس پربھونکناہےآپ خوشامدگری وچاپلوسی سےاورکتنی دولت کماناچاہتےہیں موٹرسائیکل سےپیجارو پراڈو تک کےسفرمیں ضمیرکی لاش سےاٹھتی بدبوآپ ناک بند نہیں کرتی 🙏نوٹ آپ سب سےالتجاگزارش ہےہمیں بھی فالوکریں شکریہ\n",
            "---\n",
            "Original: نہیں پائین 😎\n",
            "Processed: نہیں پائین 😎\n",
            "---\n",
            "Original:  `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی تھے '' حامد میر😁\n",
            "Processed: `` مراد علی شاہ بھیس ڈی جی آئی ایس آئی '' حامد میر😁\n",
            "---\n",
            "Original:  قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥\n",
            "Processed: قابل اعتبار ہی اکثر قاتل اعتبار ہوتے 💔🔥\n",
            "---\n",
            "Original: انساں کو تھکا دیتا ہے سوچوں کا سفر بھی ... 🍁🥀\n",
            "Processed: انساں تھکا دیتا سوچوں سفر بھی ... 🍁🥀\n",
            "---\n",
            "Original: حامد میر صاحب ویلڈن👏😊\n",
            "Processed: حامد میر صاحب ویلڈن👏😊\n",
            "---\n",
            "Original: یار وچارہ ویلا ہوندا ہے اس آرے لگا ہویا ہے😂😂 تسی تے پکے نجومی ہو اسی منندے ہاں\n",
            "Processed: یار وچارہ ویلا ہوندا اس آرے لگا ہویا ہے😂😂 تسی تے پکے نجومی اسی منندے ہاں\n",
            "---\n",
            "Original: یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے 😂😂😂\n",
            "Processed: سمجھتے سارا پاکستان بیوقوف ھے 😂😂😂\n",
            "---\n",
            "Original: تسی لڑاںٔی کروانی ساڈی کی 😂😂😂\n",
            "Processed: تسی لڑاںٔی کروانی ساڈی 😂😂😂\n",
            "---\n",
            "Original: پائن دوبارہ فالو کرئیے..؟؟😆🙄\n",
            "Processed: پائن دوبارہ فالو کرئیے..؟؟😆🙄\n",
            "---\n",
            "Original: کتنی مہنگائی ہے الو دوسو روپے درجن کدو 80روپے گز 😭😭😭 ISPR\n",
            "Processed: کتنی مہنگائی الو دوسو روپے درجن کدو 80روپے گز 😭😭😭 ISPR\n",
            "---\n",
            "Original: 😍عشق جب تم کو راس آۓ گا 💔زخم کھاٶ گے 😊مُسکراٶ گے\n",
            "Processed: 😍عشق جب راس آۓ گا 💔زخم کھاٶ گے 😊مُسکراٶ گے\n",
            "---\n",
            "Original: چونا ایسا ہی ہوتا 😂\n",
            "Processed: چونا ایسا ہی ہوتا 😂\n",
            "---\n",
            "Original: خاتم_النبیین_محمدﷺ Surat 73 سورة المزمل Ayt 20 حصہ5 ترجمہ اور کچھ اللہ کی راہ میں لڑتے ہوں گے تو جتنا قرآن میسر ہو پڑھواورنماز قائم رکھواورزکوٰة دواور اللہ کو اچھا قرض دو 🌹🌹 درود_وقرآن صَلَّی اللہُ عَلٰی حَبِیْبِہٖ سیِّدِنَا مُحَمَّدٍ وَّآلِہٖ وَصَحْبِہٖ وَسَلَّمْ\n",
            "Processed: خاتم_النبیین_محمدﷺ Surat 73 سورة المزمل Ayt 20 حصہ5 ترجمہ کچھ اللہ راہ لڑتے گے تو جتنا قرآن میسر پڑھواورنماز قائم رکھواورزکوٰة دواور اللہ اچھا قرض دو 🌹🌹 درود_وقرآن صَلَّی اللہُ عَلٰی حَبِیْبِہٖ سیِّدِنَا مُحَمَّدٍ وَّآلِہٖ وَصَحْبِہٖ وَسَلَّمْ\n",
            "---\n",
            "Original: اب بس بھی کرو بیچارے کی پہلے ہی دو بیویاں ہیے کیا اپ نے تیسری کے لیے ٹرائ کرنا ہے 😜\n",
            "Processed: اب بس بھی کرو بیچارے پہلے ہی دو بیویاں ہیے کیا اپ تیسری لیے ٹرائ کرنا 😜\n",
            "---\n",
            "Original: پتہ نہیں کیا ہورہا ہے سی سی کی بورڈ کو زنگ لگ گیا ہے😭😭\n",
            "Processed: پتہ نہیں کیا ہورہا سی سی بورڈ زنگ لگ گیا ہے😭😭\n",
            "---\n",
            "Original: اللہ آپ کو اپنی رحمتوں کے سائے میں رکھے اور مکمل صحت یاب فرمائے 🙏\n",
            "Processed: اللہ آپ اپنی رحمتوں سائے رکھے مکمل صحت یاب فرمائے 🙏\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.15.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8jW7fd5cQNy",
        "outputId": "6fa1a178-f377-4a15-b396-27108a7b1351"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorstore 0.1.67 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BSNX_KzetLS",
        "outputId": "73dc0493-9f33-4c46-c02d-54e33a71169e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.4.0\n",
            "    Uninstalling typeguard-4.4.0:\n",
            "      Successfully uninstalled typeguard-4.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.4.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install urduhack nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL-XM55iewaR",
        "outputId": "93b2e19e-2440-485f-db3f-e9f7bd810b1b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting urduhack\n",
            "  Downloading urduhack-1.1.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting tf2crf (from urduhack)\n",
            "  Downloading tf2crf-0.1.33-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow-datasets~=3.1 (from urduhack)\n",
            "  Downloading tensorflow_datasets-3.2.1-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting Click~=7.1 (from urduhack)\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from urduhack) (2024.9.11)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.4.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (24.2.0)\n",
            "Collecting dill (from tensorflow-datasets~=3.1->urduhack)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.26.4)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (3.20.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.16.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (2.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.14.1)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tf2crf->urduhack) (2.15.0)\n",
            "Requirement already satisfied: tensorflow-addons>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from tf2crf->urduhack) (0.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (2024.8.30)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (24.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (4.12.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.15.0)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons>=0.8.2->tf2crf->urduhack) (2.13.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.1.0->tf2crf->urduhack) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (3.0.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (3.2.2)\n",
            "Downloading urduhack-1.1.1-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.5/105.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_datasets-3.2.1-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf2crf-0.1.33-py2.py3-none-any.whl (7.3 kB)\n",
            "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill, Click, tensorflow-datasets, tf2crf, urduhack\n",
            "  Attempting uninstall: Click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "  Attempting uninstall: tensorflow-datasets\n",
            "    Found existing installation: tensorflow-datasets 4.9.6\n",
            "    Uninstalling tensorflow-datasets-4.9.6:\n",
            "      Successfully uninstalled tensorflow-datasets-4.9.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 2.2.5 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "dask 2024.10.0 requires click>=8.1, but you have click 7.1.2 which is incompatible.\n",
            "typer 0.12.5 requires click>=8.0.0, but you have click 7.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Click-7.1.2 dill-0.3.9 tensorflow-datasets-3.2.1 tf2crf-0.1.33 urduhack-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from urduhack.normalization import normalize\n",
        "from nltk.stem import RSLPStemmer\n",
        "# Download the stemmer resources from nltk\n",
        "nltk.download('rslp')\n",
        "# Initialize the RSLP Stemmer from NLTK (works well for multiple languages)\n",
        "stemmer = RSLPStemmer()\n",
        "# Define a list of Urdu stopwords\n",
        "urdu_stopwords = [\n",
        "    \"اور\", \"یہ\", \"کہ\", \"ہے\", \"میں\", \"کی\", \"کا\", \"کے\", \"کو\", \"نے\", \"ہوں\",\n",
        "    \"ہیں\", \"تھا\", \"تھی\", \"تھے\", \"ہو\", \"ہوا\", \"ہوئ\", \"ہوئی\", \"پر\", \"کا\"\n",
        "]\n",
        "# Dictionary for lemmatization (using a more comprehensive example)\n",
        "lemma_dict = {\n",
        "    \"چل رہی\": \"چل\",\n",
        "    \"چلتے\": \"چل\",\n",
        "    \"چلتا\": \"چل\",\n",
        "    \"اچھا\": \"اچھا\",\n",
        "    \"اچھی\": \"اچھا\",\n",
        "    \"اچھے\": \"اچھا\",\n",
        "    \"کرتے\": \"کر\",\n",
        "    \"کرتا\": \"کر\",\n",
        "    \"کررہے\": \"کر\",\n",
        "    \"گئے\": \"جانا\",\n",
        "    \"کرے\": \"کر\",\n",
        "    \"دیکھتے\": \"دیکھ\",\n",
        "    \"دیکھا\": \"دیکھ\",\n",
        "    \"دیکھ\": \"دیکھ\",\n",
        "    \"جا\": \"جانا\",\n",
        "    \"کھا\": \"کھانا\",\n",
        "    \"پڑھا\": \"پڑھنا\",\n",
        "    \"چلنا\": \"چل\",\n",
        "    \"بولا\": \"بول\",\n",
        "    \"سنا\": \"سننا\"\n",
        "}\n",
        "def remove_stopwords(text, stopwords):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stopwords]\n",
        "    return \" \".join(filtered_words)\n",
        "def urdu_lemmatizer(word):\n",
        "    return lemma_dict.get(word, word)  # Use the lemma if available, otherwise return the original word\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Step 1: Normalize text using UrduHack\n",
        "    text = normalize(text)\n",
        "    # Step 2: Remove URLs, punctuation, and unwanted characters\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'#\\w+', '', text)     # Remove hashtags\n",
        "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
        "    # Step 3: Remove stopwords\n",
        "    processed_text = remove_stopwords(text, urdu_stopwords)\n",
        "    # Step 4: Stemming using NLTK RSLP Stemmer\n",
        "    stemmed_words = [stemmer.stem(word) for word in processed_text.split()]\n",
        "    stemmed_text = \" \".join(stemmed_words)\n",
        "    # Step 5: Lemmatization using custom dictionary\n",
        "    lemmatized_words = [urdu_lemmatizer(word) for word in stemmed_text.split()]\n",
        "    lemmatized_text = \" \".join(lemmatized_words)\n",
        "    return lemmatized_text\n",
        "# Open the file and read its content\n",
        "file_path = 'urdu_sarcastic_dataset.csv'\n",
        "with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    for i, row in enumerate(csv_reader):\n",
        "        if i >= 20:  # Process first 100 rows for testing (adjust as needed)\n",
        "            break\n",
        "        if row:\n",
        "            # Assuming the text is in the first column (index 0)\n",
        "            text = row[0]\n",
        "            preprocessed_text = preprocess_text(text)\n",
        "            print(f\"Original: {text}\")\n",
        "            print(f\"Preprocessed: {preprocessed_text}\")\n",
        "            print(\"---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYjM28sXWODV",
        "outputId": "1780d00e-f73d-4cca-fcc5-52ac7e9e14e6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: ﻿urdu_text\n",
            "Preprocessed: urdu_text\n",
            "---\n",
            "Original: 🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہیں چاہیے 😐😐😐🤣\n",
            "Preprocessed: لینے دے میری شادی فسادن ٹھیک کوجی نہیں چاہیے\n",
            "---\n",
            "Original: چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی آں میں😂😂\n",
            "Preprocessed: چل مہمانوں کھانا سرو کر چڑیل چاچی نوں دسدی آں\n",
            "---\n",
            "Original: کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپوزیشن کی کردار کشی اوراس پربھونکناہےآپ خوشامدگری وچاپلوسی سےاورکتنی دولت کماناچاہتےہیں موٹرسائیکل سےپیجارو پراڈو تک کےسفرمیں ضمیرکی لاش سےاٹھتی بدبوآپ کی ناک بند نہیں کرتی ہے 🙏نوٹ آپ سب سےالتجاگزارش ہےہمیں بھی فالوکریں شکریہ\n",
            "Preprocessed: کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپوزیشن کردار کشی اوراس پربھونکناہےآپ خوشامدگری وچاپلوسی سےاورکتنی دولت کماناچاہتےہیں موٹرسائیکل سےپیجارو پراڈو تک کےسفرمیں ضمیرکی لاش سےاٹھتی بدبوآپ ناک بند نہیں کرتی نوٹ آپ سب سےالتجاگزارش ہےہمیں بھی فالوکریں شکریہ\n",
            "---\n",
            "Original: نہیں پائین 😎\n",
            "Preprocessed: نہیں پائین\n",
            "---\n",
            "Original:  `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی تھے '' حامد میر😁\n",
            "Preprocessed: مراد علی شاہ بھیس ڈی جی آئی ایس آئی حامد میر\n",
            "---\n",
            "Original:  قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥\n",
            "Preprocessed: قابل اعتبار ہی اکثر قاتل اعتبار ہوتے\n",
            "---\n",
            "Original: انساں کو تھکا دیتا ہے سوچوں کا سفر بھی ... 🍁🥀\n",
            "Preprocessed: انساں تھکا دیتا سوچوں سفر بھی\n",
            "---\n",
            "Original: حامد میر صاحب ویلڈن👏😊\n",
            "Preprocessed: حامد میر صاحب ویلڈن\n",
            "---\n",
            "Original: یار وچارہ ویلا ہوندا ہے اس آرے لگا ہویا ہے😂😂 تسی تے پکے نجومی ہو اسی منندے ہاں\n",
            "Preprocessed: یار وچارہ ویلا ہوندا اس آرے لگا ہویا تسی تے پکے نجومی اسی منندے ہاں\n",
            "---\n",
            "Original: یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے 😂😂😂\n",
            "Preprocessed: سمجھتے سارا پاکستان بیوقوف ھے\n",
            "---\n",
            "Original: تسی لڑاںٔی کروانی ساڈی کی 😂😂😂\n",
            "Preprocessed: تسی لڑاںی کروانی ساڈی\n",
            "---\n",
            "Original: پائن دوبارہ فالو کرئیے..؟؟😆🙄\n",
            "Preprocessed: پائن دوبارہ فالو کرئیے\n",
            "---\n",
            "Original: کتنی مہنگائی ہے الو دوسو روپے درجن کدو 80روپے گز 😭😭😭 ISPR\n",
            "Preprocessed: کتنی مہنگائی الو دوسو روپے درجن کدو 80روپے گز ispr\n",
            "---\n",
            "Original: 😍عشق جب تم کو راس آۓ گا 💔زخم کھاٶ گے 😊مُسکراٶ گے\n",
            "Preprocessed: عشق جب تم راس آۓ گا زخم کھاٶ گے مسکراٶ گے\n",
            "---\n",
            "Original: چونا ایسا ہی ہوتا 😂\n",
            "Preprocessed: چونا ایسا ہی ہوتا\n",
            "---\n",
            "Original: خاتم_النبیین_محمدﷺ Surat 73 سورة المزمل Ayt 20 حصہ5 ترجمہ اور کچھ اللہ کی راہ میں لڑتے ہوں گے تو جتنا قرآن میسر ہو پڑھواورنماز قائم رکھواورزکوٰة دواور اللہ کو اچھا قرض دو 🌹🌹 درود_وقرآن صَلَّی اللہُ عَلٰی حَبِیْبِہٖ سیِّدِنَا مُحَمَّدٍ وَّآلِہٖ وَصَحْبِہٖ وَسَلَّمْ\n",
            "Preprocessed: خاتم_النبیین_محمدﷺ surat 73 سورۃ المزمل ayt 20 حصہ5 ترجمہ کچھ اللہ راہ لڑتے گے تو جتنا قرآن میسر پڑھواورنماز قائم رکھواورزکوۃ دواور اللہ اچھا قرض دو درود_وقرآن صلی اللہ علی حبیبہ سیدنا محمد وآلہ وصحبہ وسلم\n",
            "---\n",
            "Original: اب بس بھی کرو بیچارے کی پہلے ہی دو بیویاں ہیے کیا اپ نے تیسری کے لیے ٹرائ کرنا ہے 😜\n",
            "Preprocessed: اب بس بھی کرو بیچارے پہلے ہی دو بیویاں ہیے کیا اپ تیسری لیے ٹرائ کرنا\n",
            "---\n",
            "Original: پتہ نہیں کیا ہورہا ہے سی سی کی بورڈ کو زنگ لگ گیا ہے😭😭\n",
            "Preprocessed: پتہ نہیں کیا ہورہا سی سی بورڈ زنگ لگ گیا\n",
            "---\n",
            "Original: اللہ آپ کو اپنی رحمتوں کے سائے میں رکھے اور مکمل صحت یاب فرمائے 🙏\n",
            "Preprocessed: اللہ آپ اپنی رحمتوں سائے رکھے مکمل صحت یاب فرمائے\n",
            "---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk scikit-learn gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5CDblzfWY3e",
        "outputId": "e12898ef-3ad3-4513-e720-d1e496259de5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure required NLTK data is available for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to preprocess the text (tokenization)\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the text by tokenizing it.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be tokenized.\n",
        "\n",
        "    Returns:\n",
        "        list: List of tokens.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text)  # Tokenize the text using NLTK's word_tokenize\n",
        "    return tokens\n",
        "\n",
        "# Open the CSV file and read its content using Python's built-in csv module\n",
        "file_path = 'urdu_sarcastic_dataset.csv'  # Specify the path to the CSV file\n",
        "preprocessed_texts = []  # List to store preprocessed text data\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "\n",
        "    # Iterate over each row in the CSV\n",
        "    for i, row in enumerate(csv_reader):\n",
        "        if i >= 5500:  # Process a limited number of rows (adjust as needed)\n",
        "            break\n",
        "\n",
        "        # Check if the row contains valid text data\n",
        "        if row and isinstance(row[0], str):\n",
        "            text = row[0]  # Assume the text is in the first column\n",
        "            # Preprocess the text and append it to the list\n",
        "            preprocessed_texts.append(\" \".join(preprocess_text(text)))\n",
        "\n",
        "# Tokenization for Word2Vec training (re-tokenize the preprocessed texts)\n",
        "tokenized_texts = [preprocess_text(text) for text in preprocessed_texts]\n",
        "\n",
        "# TF-IDF Feature Extraction\n",
        "vectorizer = TfidfVectorizer()  # Initialize the TF-IDF vectorizer\n",
        "tfidf_matrix = vectorizer.fit_transform(preprocessed_texts)  # Fit and transform the text data\n",
        "\n",
        "# Get feature names (words) from the TF-IDF model\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix to a dense format for easier manipulation\n",
        "dense = tfidf_matrix.todense()\n",
        "denselist = dense.tolist()\n",
        "\n",
        "# Get the top 10 words with highest TF-IDF scores across the dataset\n",
        "tfidf_scores = [(feature_names[col], score) for row in denselist for col, score in enumerate(row)]\n",
        "tfidf_scores.sort(key=lambda x: x[1], reverse=True)  # Sort the words by their TF-IDF scores\n",
        "top_10_tfidf_words = tfidf_scores[:10]  # Get the top 10 words\n",
        "\n",
        "# Print the top 10 words based on TF-IDF scores\n",
        "print(\"Top 10 TF-IDF words:\")\n",
        "for word, score in top_10_tfidf_words:\n",
        "    print(f\"{word}: {score}\")\n",
        "\n",
        "# Word2Vec Training\n",
        "# Initialize and train the Word2Vec model with tokenized text\n",
        "model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=20, min_count=1, workers=7)\n",
        "model.train(tokenized_texts, total_examples=len(tokenized_texts), epochs=10)  # Train the model\n",
        "\n",
        "# Find the 5 words most similar to \"اچھا\" (good)\n",
        "try:\n",
        "    similar_words = model.wv.most_similar(\"اچھا\", topn=5)  # Find the top 5 most similar words\n",
        "    print(\"\\n5 words closest to 'اچھا':\")\n",
        "    for word, similarity in similar_words:\n",
        "        print(f\"{word}: {similarity}\")\n",
        "\n",
        "except KeyError:\n",
        "    print(\"\\n'اچھا' not found in the vocabulary.\")  # Handle case where the word is not in the model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvkdH4oze1iK",
        "outputId": "c223c92a-2795-4cfe-dcfd-e6ce0bdd39e1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 TF-IDF words:\n",
            "urdu_text: 1.0\n",
            "ڈرامی: 1.0\n",
            "استغفراللہ: 1.0\n",
            "اکاؤنٹ: 1.0\n",
            "اھاں: 1.0\n",
            "ههههههههههههههههههههہ: 1.0\n",
            "ہاہاہا: 1.0\n",
            "اہو: 1.0\n",
            "آہو: 1.0\n",
            "توبہ: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5 words closest to 'اچھا':\n",
            "اتنا: 0.9929912686347961\n",
            "ویسے: 0.9843923449516296\n",
            "ایسا: 0.9698293805122375\n",
            "تمہیں: 0.9675552248954773\n",
            "کیڑے: 0.9665562510490417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "# Ensure required NLTK data is available for tokenization\n",
        "nltk.download('punkt')\n",
        "def preprocess_text(text):\n",
        "    return nltk.word_tokenize(text)\n",
        "def generate_ngrams(tokens, n):\n",
        "    return list(ngrams(tokens, n))\n",
        "def read_csv(file_path):\n",
        "    texts = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file)\n",
        "        # Iterate through each row in the CSV\n",
        "        for i, row in enumerate(csv_reader):\n",
        "            if i >= 20:  # Limit the number of rows to process (adjust as necessary)\n",
        "                break\n",
        "            # Ensure the row contains valid text data\n",
        "            if row and isinstance(row[0], str):\n",
        "                text = row[0]  # Assuming the text is in the first column\n",
        "                texts.append(text)\n",
        "    return texts\n",
        "def compute_ngram_frequencies(texts, n):\n",
        "    all_ngrams = []\n",
        "    # Iterate through each text, tokenize it, and generate n-grams\n",
        "    for text in texts:\n",
        "        tokens = preprocess_text(text)\n",
        "        ngrams_list = generate_ngrams(tokens, n)\n",
        "        all_ngrams.extend(ngrams_list)  # Add n-grams to the collection\n",
        "    return Counter(all_ngrams)  # Count frequencies of each n-gram\n",
        "def print_top_ngrams(ngram_counts, n=10):\n",
        "    top_ngrams = ngram_counts.most_common(n)  # Get the top N most common n-grams\n",
        "    for ngram, freq in top_ngrams:\n",
        "        print(f\"{ngram}: {freq}\")  # Print each n-gram and its frequency\n",
        "if __name__ == \"__main__\":\n",
        "    # File path to the Urdu text CSV\n",
        "    file_path = 'urdu_sarcastic_dataset.csv'  # Specify the path to your CSV file\n",
        "    # Read the CSV and get the texts (up to 5500 rows)\n",
        "    texts = read_csv(file_path)\n",
        "    # Compute bigram and trigram frequencies\n",
        "    bigram_counts = compute_ngram_frequencies(texts, 2)  # Compute frequencies for bigrams\n",
        "    trigram_counts = compute_ngram_frequencies(texts, 3)  # Compute frequencies for trigrams\n",
        "    # Display the top 10 bigrams\n",
        "    print(\"Top 10 Bigrams:\")\n",
        "    print_top_ngrams(bigram_counts)\n",
        "    # Display the top 10 trigrams\n",
        "    print(\"\\nTop 10 Trigrams:\")\n",
        "    print_top_ngrams(trigram_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNMT3g6FVX6W",
        "outputId": "4db2cdad-a2f6-48ce-db8d-5ac8f26c859f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Bigrams:\n",
            "('🤣😂😂', 'ہو'): 1\n",
            "('ہو', 'لینے'): 1\n",
            "('لینے', 'دے'): 1\n",
            "('دے', 'میری'): 1\n",
            "('میری', 'شادی'): 1\n",
            "('شادی', 'فسادن'): 1\n",
            "('فسادن', 'ٹھیک'): 1\n",
            "('ٹھیک', 'ہے'): 1\n",
            "('ہے', 'کوجی'): 1\n",
            "('کوجی', 'نہیں'): 1\n",
            "\n",
            "Top 10 Trigrams:\n",
            "('🤣😂😂', 'ہو', 'لینے'): 1\n",
            "('ہو', 'لینے', 'دے'): 1\n",
            "('لینے', 'دے', 'میری'): 1\n",
            "('دے', 'میری', 'شادی'): 1\n",
            "('میری', 'شادی', 'فسادن'): 1\n",
            "('شادی', 'فسادن', 'ٹھیک'): 1\n",
            "('فسادن', 'ٹھیک', 'ہے'): 1\n",
            "('ٹھیک', 'ہے', 'کوجی'): 1\n",
            "('ہے', 'کوجی', 'نہیں'): 1\n",
            "('کوجی', 'نہیں', 'چاہیے'): 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "# Ensure required NLTK data is available for tokenization\n",
        "nltk.download('punkt')\n",
        "# Function to preprocess the text (tokenization)\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "# Path to the CSV file containing the Urdu text\n",
        "file_path = 'urdu_sarcastic_dataset.csv'  # Change this to your actual file path\n",
        "preprocessed_texts = []  # List to store preprocessed texts\n",
        "# Read the CSV file and preprocess each row\n",
        "with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    # Iterate over each row in the CSV file\n",
        "    for i, row in enumerate(csv_reader):\n",
        "        if i >= 20:  # Limit the number of rows to process (adjust as necessary)\n",
        "            break\n",
        "        if row and isinstance(row[0], str):  # Ensure the row contains valid text\n",
        "            text = row[0]  # Assuming the text is in the first column\n",
        "            preprocessed_texts.append(\" \".join(preprocess_text(text)))  # Tokenize and join tokens back into a string\n",
        "# Assuming you have a list of sentiment labels (0 for negative, 1 for positive)\n",
        "labels = np.random.randint(2, size=len(preprocessed_texts))  # Random labels, replace with actual labels from dataset\n",
        "# Option 1: TF-IDF Feature Extraction\n",
        "vectorizer = TfidfVectorizer()  # Initialize the TF-IDF vectorizer\n",
        "tfidf_matrix = vectorizer.fit_transform(preprocessed_texts)  # Fit and transform the preprocessed texts\n",
        "# Option 2: Word2Vec Feature Extraction\n",
        "tokenized_texts = [preprocess_text(text) for text in preprocessed_texts]  # Re-tokenize the preprocessed texts\n",
        "word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)  # Train Word2Vec model\n",
        "# Calculate the average word vector for each text\n",
        "X_word2vec = np.array([np.mean([word2vec_model.wv[word] for word in words if word in word2vec_model.wv] or [np.zeros(100)], axis=0) for words in tokenized_texts])\n",
        "# Use TF-IDF matrix for the model (you can switch to X_word2vec for Word2Vec features)\n",
        "X = tfidf_matrix  # Use TF-IDF features (or replace with X_word2vec for Word2Vec features)\n",
        "y = labels  # Target labels\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Initialize the Logistic Regression model\n",
        "classifier = LogisticRegression(max_iter=1000)  # Set max_iter to ensure convergence\n",
        "# Train the model on the training set\n",
        "classifier.fit(X_train, y_train)\n",
        "# Make predictions on the test set\n",
        "y_pred = classifier.predict(X_test)\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy\n",
        "precision = precision_score(y_test, y_pred, average='weighted')  # Calculate precision\n",
        "recall = recall_score(y_test, y_pred, average='weighted')  # Calculate recall\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')  # Calculate F1-score\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_zHUO2_bwB0",
        "outputId": "4310955b-ebf5-40d2-fd2d-f84644be1e19"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2500\n",
            "Precision: 0.0625\n",
            "Recall: 0.2500\n",
            "F1-Score: 0.1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Assuming X (features) and y (labels) are already prepared (e.g., from TF-IDF or Word2Vec features and sentiment labels)\n",
        "# Splitting the data into training and validation sets (80% training, 20% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Initialize the logistic regression classifier and train it on the training set\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train, y_train)\n",
        "# Make predictions on the validation set\n",
        "y_val_pred = classifier.predict(X_val)\n",
        "# Calculate evaluation metrics on the validation set\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)  # Calculate accuracy\n",
        "val_precision = precision_score(y_val, y_val_pred, average='weighted')  # Calculate precision (weighted average)\n",
        "val_recall = recall_score(y_val, y_val_pred, average='weighted')  # Calculate recall (weighted average)\n",
        "val_f1 = f1_score(y_val, y_val_pred, average='weighted')  # Calculate F1-score (weighted average)\n",
        "# Print the validation metrics\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"Validation Precision: {val_precision:.4f}\")\n",
        "print(f\"Validation Recall: {val_recall:.4f}\")\n",
        "print(f\"Validation F1-Score: {val_f1:.4f}\")\n",
        "# Generate the confusion matrix to evaluate classification performance\n",
        "conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
        "# Plot the confusion matrix using seaborn for better visualization\n",
        "plt.figure(figsize=(8, 6))  # Set figure size\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])  # Create heatmap\n",
        "plt.title('Confusion Matrix')  # Set plot title\n",
        "plt.xlabel('Predicted Label')  # Label x-axis as Predicted Label\n",
        "plt.ylabel('True Label')  # Label y-axis as True Label\n",
        "plt.show()  # Display the confusion matrix\n",
        "# Generate a detailed classification report to evaluate precision, recall, and F1-score for each class\n",
        "report = classification_report(y_val, y_val_pred, target_names=['Negative', 'Positive'])\n",
        "print(\"Classification Report:\\n\", report)\n",
        "# Identify and print the first 5 misclassified examples for further analysis\n",
        "print(\"\\nMisclassified Examples:\")\n",
        "misclassified_indices = np.where(y_val != y_val_pred)[0]  # Find indices where predictions and true labels don't match\n",
        "for index in misclassified_indices[:5]:  # Limit to first 5 misclassified examples\n",
        "    print(f\"Example {index}:\")\n",
        "    print(f\"Text: {preprocessed_texts[index]}\")  # Print the text of the misclassified example\n",
        "    print(f\"Actual Label: {y_val[index]}, Predicted Label: {y_val_pred[index]}\")  # Print the actual and predicted labels\n",
        "    print(\"---\")\n",
        "# Identify patterns or challenges in the misclassified examples\n",
        "print(\"\\nPotential Challenges in Urdu Sentiment Analysis based on misclassified examples:\")\n",
        "for index in misclassified_indices[:5]:  # Limit to first 5 examples for readability\n",
        "    text = preprocessed_texts[index]  # Get the misclassified text\n",
        "    actual_label = y_val[index]  # Get the actual label\n",
        "    predicted_label = y_val_pred[index]  # Get the predicted label\n",
        "    if predicted_label != actual_label:  # Analyze only misclassified cases\n",
        "        print(f\"Example {index} shows a challenge with:\")\n",
        "        # Check for complex sentence structure (example logic: if sentence is longer than 15 words)\n",
        "        if len(text.split()) > 15:\n",
        "            print(\"  - Complex sentence structure\")\n",
        "        # Check for possible sarcasm or ambiguous sentiment (example logic: check for exclamation or question marks)\n",
        "        if \"!\" in text or \"؟\" in text:  # \"؟\" is a common Urdu question mark\n",
        "            print(\"  - Possible sarcasm or ambiguous sentiment\")\n",
        "        # Detect colloquial language or slang (example logic: check for specific Urdu slang words)\n",
        "        if any(word in text for word in ['یار', 'لوگ', 'کیا']):  # Example Urdu slang/colloquial words\n",
        "            print(\"  - Colloquial language or slang\")\n",
        "        print(\"---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OwkvVlg_ddIf",
        "outputId": "5c9df6b5-51e0-4dea-81fa-85c86b6b68de"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.2500\n",
            "Validation Precision: 0.0625\n",
            "Validation Recall: 0.2500\n",
            "Validation F1-Score: 0.1000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00         3\n",
            "    Positive       0.25      1.00      0.40         1\n",
            "\n",
            "    accuracy                           0.25         4\n",
            "   macro avg       0.12      0.50      0.20         4\n",
            "weighted avg       0.06      0.25      0.10         4\n",
            "\n",
            "\n",
            "Misclassified Examples:\n",
            "Example 1:\n",
            "Text: 🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہیں چاہیے 😐😐😐🤣\n",
            "Actual Label: 0, Predicted Label: 1\n",
            "---\n",
            "Example 2:\n",
            "Text: چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی آں میں😂😂\n",
            "Actual Label: 0, Predicted Label: 1\n",
            "---\n",
            "Example 3:\n",
            "Text: کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپوزیشن کی کردار کشی اوراس پربھونکناہےآپ خوشامدگری وچاپلوسی سےاورکتنی دولت کماناچاہتےہیں موٹرسائیکل سےپیجارو پراڈو تک کےسفرمیں ضمیرکی لاش سےاٹھتی بدبوآپ کی ناک بند نہیں کرتی ہے 🙏نوٹ آپ سب سےالتجاگزارش ہےہمیں بھی فالوکریں شکریہ\n",
            "Actual Label: 0, Predicted Label: 1\n",
            "---\n",
            "\n",
            "Potential Challenges in Urdu Sentiment Analysis based on misclassified examples:\n",
            "Example 1 shows a challenge with:\n",
            "---\n",
            "Example 2 shows a challenge with:\n",
            "---\n",
            "Example 3 shows a challenge with:\n",
            "  - Complex sentence structure\n",
            "---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}